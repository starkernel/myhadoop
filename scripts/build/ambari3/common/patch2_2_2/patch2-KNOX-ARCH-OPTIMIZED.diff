Subject: [PATCH] fixed：2.2.2
---
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CLOUDBEAVER/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CLOUDBEAVER/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CLOUDBEAVER/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CLOUDBEAVER/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CLOUDBEAVER/metainfo.xml	(date 1765958707589)
@@ -22,7 +22,7 @@
     <service>
       <name>CLOUDBEAVER</name>
       <displayName>Cloudbeaver</displayName>
-      <comment>Component CLOUDBEAVER Integrated By JaneTTR . For commercial use, please contact mail: 3832514048@qq.com</comment>
+      <comment>【增强】通用数据库管理与运维服务，提供可视化的数据库连接、查询与管理能力，使用前需安装并部署目标数据库及相关驱动</comment>
       <version>24.3.3</version>
       <components>        
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KERBEROS/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KERBEROS/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KERBEROS/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KERBEROS/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KERBEROS/metainfo.xml	(date 1765958707156)
@@ -21,12 +21,8 @@
     <service>
       <name>KERBEROS</name>
       <displayName>Kerberos</displayName>
-      <comment>A computer network authentication protocol which works on
-        the basis of 'tickets' to allow nodes communicating over a
-        non-secure network to prove their identity to one another in a
-        secure manner.
-      </comment>
-      <version>1.10.3-30</version>
+      <comment>【增强】集中式身份认证与安全服务，用于集群组件的统一认证与访问控制，使用手册及踩坑可参考 https://doc.janettr.com/install/kerberos/</comment>
+      <version>1.18.2</version>
 
       <components>
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DORIS/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DORIS/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DORIS/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DORIS/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DORIS/metainfo.xml	(date 1765958704297)
@@ -21,7 +21,7 @@
         <service>
             <name>DORIS</name>
             <displayName>Doris2</displayName>
-            <comment>Component DORIS2 Integrated By JaneTTR . For commercial use, please contact mail: 3832514048@qq.com</comment>
+            <comment>【增强】MPP 分析型数据库服务，提供高并发、低延迟的分析查询能力，使用前需安装并部署 MySQL、HDFS 等依赖组件</comment>
             <version>2.1.7</version>
             <components>
                 <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml	(date 1765958704841)
@@ -21,7 +21,7 @@
         <service>
             <name>KAFKA</name>
             <displayName>Kafka</displayName>
-            <comment>A high-throughput distributed messaging system</comment>
+            <comment>【增强】分布式消息队列与流数据平台，用于高吞吐、低延迟的数据发布与订阅，使用前需安装并部署 ZooKeeper 等依赖组件</comment>
             <version>2.8.1</version>
             <components>
                 <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/metainfo.xml	(date 1765958706105)
@@ -20,7 +20,7 @@
         <service>
             <name>TRINO</name>
             <displayName>Trino</displayName>
-            <comment>Component Trino Integrated By JaneTTR . For commercial use, please contact mail: 3832514048@qq.com</comment>
+            <comment>【增强】分布式 MPP SQL 查询引擎，用于对多种数据源进行交互式分析查询，使用前需安装并部署相关数据源及元数据服务</comment>
             <version>474</version>
 			<selection>TECH_PREVIEW</selection>
             <components>
@@ -127,7 +127,13 @@
                 </quickLinksConfiguration>
             </quickLinksConfigurations>
 
-			
+            <themes>
+                <theme>
+                    <fileName>theme.json</fileName>
+                    <default>true</default>
+                </theme>
+            </themes>
+
         </service>
     </services>
 </metainfo>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/metainfo.xml	(date 1765958700063)
@@ -22,7 +22,7 @@
       <name>HDFS</name>
       <displayName>HDFS</displayName>
       <serviceType>HDFS</serviceType> <!-- This tag is used only for main fileSystem service. It sets filesystem schema for ambari -->
-      <comment>Apache Hadoop Distributed File System</comment>
+      <comment>【基础】分布式文件系统，用于存储和管理海量数据，提供高可靠性与高吞吐的数据访问能力，使用前需安装并部署 Java 等基础运行环境</comment>
       <version>3.3.4</version>
 
       <components>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SUPERSET/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SUPERSET/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SUPERSET/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SUPERSET/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SUPERSET/metainfo.xml	(date 1765958707214)
@@ -21,7 +21,7 @@
     <service>
       <name>SUPERSET</name>
       <displayName>Superset</displayName>
-      <comment>Component SUPERSET Integrated By JaneTTR . For commercial use, please contact mail: 3832514048@qq.com</comment>
+      <comment>【增强】数据可视化与交互式分析服务，用于构建数据仪表盘与分析报表，使用前需安装并部署数据库及相关认证组件</comment>
       <version>4.1.2</version>
       <components>
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/themes/theme.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/themes/theme.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/themes/theme.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/themes/theme.json	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/themes/theme.json	(date 1765958705994)
@@ -1,6 +1,6 @@
 {
   "name": "default",
-  "description": "Default theme for Presto service",
+  "description": "Default theme for Trino service",
   "configuration": {
     "layouts": [
       {
@@ -91,96 +91,100 @@
       "configuration-layout": "default",
       "configs": [
         {
-          "config": "node.properties/node.environment",
+          "config": "node-properties/node.environment",
+          "subsection-name": "subsection-node-config"
+        },
+        {
+          "config": "config-properties/web-ui.authentication.type",
           "subsection-name": "subsection-node-config"
         },
         {
-          "config": "node.properties/plugin.config-dir",
+          "config": "node-properties/plugin.config-dir",
           "subsection-name": "subsection-node-config"
         },
         {
-          "config": "node.properties/plugin.dir",
+          "config": "node-properties/plugin.dir",
           "subsection-name": "subsection-node-config"
         },
         {
-          "config": "connectors.properties/connectors.to.add",
+          "config": "connectors-properties/connectors.to.add",
           "subsection-name": "subsection-connector-config"
         },
         {
-          "config": "connectors.properties/connectors.to.delete",
+          "config": "connectors-properties/connectors.to.delete",
           "subsection-name": "subsection-connector-config"
         },
         {
-          "config": "config.properties/node-scheduler.include-coordinator",
+          "config": "config-properties/node-scheduler.include-coordinator",
           "subsection-name": "subsection-general-config"
         },
         {
-          "config": "config.properties/http-server.http.port",
+          "config": "config-properties/http-server.http.port",
           "subsection-name": "subsection-general-config"
         },
         {
-          "config": "config.properties/query.max-memory",
+          "config": "config-properties/query.max-memory-gb",
           "subsection-name": "subsection-general-config"
         },
         {
-          "config": "config.properties/query.max-memory-per-node",
+          "config": "config-properties/query.max-memory-per-node-gb",
           "subsection-name": "subsection-general-config"
         },
         {
-          "config": "config.properties/discovery.uri",
+          "config": "config-properties/discovery.uri",
           "subsection-name": "subsection-general-config"
         },
         {
-          "config": "jvm.config/jvm.config",
+          "config": "jvm-config/jvm.config",
           "subsection-name": "subsection-jvm-config"
         }
       ]
     },
     "widgets": [
       {
-        "config": "node.properties/node.environment",
+        "config": "node-properties/node.environment",
         "widget": {
           "type": "text-area"
         }
       },
       {
-        "config": "node.properties/plugin.config-dir",
+        "config": "node-properties/plugin.config-dir",
         "widget": {
           "type": "directory"
         }
       },
       {
-        "config": "node.properties/plugin.dir",
+        "config": "node-properties/plugin.dir",
         "widget": {
           "type": "directory"
         }
       },
       {
-        "config": "connectors.properties/connectors.to.add",
+        "config": "connectors-properties/connectors.to.add",
         "widget": {
           "type": "text-area"
         }
       },
       {
-        "config": "connectors.properties/connectors.to.delete",
+        "config": "connectors-properties/connectors.to.delete",
         "widget": {
           "type": "text-area"
         }
       },
       {
-        "config": "config.properties/node-scheduler.include-coordinator",
+        "config": "config-properties/node-scheduler.include-coordinator",
         "widget": {
           "type": "toggle"
         }
       },
       {
-        "config": "config.properties/http-server.http.port",
+        "config": "config-properties/http-server.http.port",
         "widget": {
           "type": "text-area"
         }
       },
       {
-        "config": "config.properties/query.max-memory",
+        "config": "config-properties/query.max-memory-gb",
         "widget": {
           "type": "slider",
           "units": [
@@ -191,7 +195,7 @@
         }
       },
       {
-        "config": "config.properties/query.max-memory-per-node",
+        "config": "config-properties/query.max-memory-per-node-gb",
         "widget": {
           "type": "slider",
           "units": [
@@ -202,16 +206,22 @@
         }
       },
       {
-        "config": "config.properties/discovery.uri",
+        "config": "config-properties/discovery.uri",
         "widget": {
           "type": "text-area"
         }
       },
       {
-        "config": "jvm.config/jvm.config",
+        "config": "jvm-config/jvm.config",
         "widget": {
           "type": "text-area"
         }
+      },
+      {
+        "config": "config-properties/web-ui.authentication.type",
+        "widget": {
+          "type": "combo"
+        }
       }
     ]
   }
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/metainfo.xml	(date 1765958704009)
@@ -22,7 +22,7 @@
     <service>
       <name>YARN</name>
       <displayName>YARN</displayName>
-      <comment>Apache Hadoop NextGen MapReduce (YARN)</comment>
+      <comment>【基础】集群资源管理与任务调度服务，用于统一管理计算资源并调度各类应用，使用前需安装并部署 HDFS、ZooKeeper 等依赖组件</comment>
       <version>3.3.4</version>
       <components>
 
@@ -221,8 +221,8 @@
     <service>
       <name>MAPREDUCE2</name>
       <displayName>MapReduce2</displayName>
-      <comment>Apache Hadoop NextGen MapReduce (YARN)</comment>
-      <version>3.3.4-1</version>
+      <comment>【基础】分布式批处理计算框架，用于在 YARN 资源管理下执行大规模离线计算任务，使用前需安装并部署 YARN、HDFS 等依赖组件</comment>
+      <version>3.3.4</version>
       <components>
         <component>
           <name>HISTORYSERVER</name>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/NIGHTINGALE/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/NIGHTINGALE/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/NIGHTINGALE/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/NIGHTINGALE/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/NIGHTINGALE/metainfo.xml	(date 1765958707660)
@@ -22,9 +22,7 @@
         <service>
             <name>NIGHTINGALE</name>
             <displayName>Nightingale</displayName>
-            <comment>Component NIGHTINGALE Integrated By JaneTTR . For commercial use, please contact mail:
-                3832514048@qq.com
-            </comment>
+            <comment>【废弃】临时废弃，待思考完毕，后续可能将以全新姿势与大家见面！</comment>
             <version>7.7.2</version>
             <components>
                 <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUDI/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUDI/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUDI/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUDI/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUDI/metainfo.xml	(date 1765958701831)
@@ -21,9 +21,7 @@
         <service>
             <name>HUDI</name>
             <displayName>Hudi</displayName>
-            <comment>Component HUDI Power By JaneTTR . mail: 3832514048@qq.com ,git:
-                https://gitee.com/tt-bigdata/ambari-env
-            </comment>
+            <comment>【增强】面向数据湖场景的增量数据管理组件，支持高效写入、更新与时间旅行查询，使用前需安装并部署 HDFS、Hive Metastore、Spark 等依赖组件</comment>
             <version>1.1.0</version>
 
             <components>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/metainfo.xml	(date 1765958706655)
@@ -25,7 +25,7 @@
             <name>RANGER</name>
             <displayName>Ranger</displayName>
             <version>2.4.0</version>
-            <comment>Component Ranger Power By JaneTTR . mail: 3832514048@qq.com ,git: https://gitee.com/tt-bigdata/ambari-env</comment>
+            <comment>【增强】集中式权限管理与安全策略服务，用于对集群组件进行统一的访问控制与审计，使用前需安装并部署数据库、Kerberos 等依赖组件</comment>
             <themes>
                 <theme>
                     <fileName>credentials.json</fileName>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/PAIMON/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/PAIMON/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/PAIMON/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/PAIMON/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/PAIMON/metainfo.xml	(date 1765958706477)
@@ -21,9 +21,7 @@
         <service>
             <name>PAIMON</name>
             <displayName>Paimon</displayName>
-            <comment>Component PAIMON Power By JaneTTR . mail: 3832514048@qq.com ,git:
-                https://gitee.com/tt-bigdata/ambari-env
-            </comment>
+            <comment>【增强】面向数据湖场景的表格式存储与管理组件，支持流批一体的数据写入与查询，使用前需安装并部署 HDFS、Hive Metastore、Flink 或 Spark 等依赖组件</comment>
             <version>1.0.1</version>
 
             <components>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/role_command_order.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/role_command_order.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/role_command_order.json
new file mode 100644
--- /dev/null	(date 1765958703044)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/role_command_order.json	(date 1765958703044)
@@ -0,0 +1,7 @@
+{
+  "general_deps" : {
+    "_comment" : "dependencies for KNOX",
+    "KNOX_GATEWAY-START" : ["RANGER_USERSYNC-START", "NAMENODE-START"],
+    "KNOX_SERVICE_CHECK-SERVICE_CHECK" : ["KNOX_GATEWAY-START"]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/configuration/config-properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/configuration/config-properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/configuration/config-properties.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/configuration/config-properties.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/configuration/config-properties.xml	(date 1765958706253)
@@ -76,31 +76,44 @@
     </property>
 
     <property>
-        <name>query.max-memory</name>
-        <value>2GB</value>
+        <name>query.max-memory-gb</name>
+        <display-name>Query Max Memory (GB)</display-name>
+        <value>2</value>
         <description>
-            The maximum amount of distributed memory that a query may use.
-
-            If you'd like to enter a value higher than the maximum on the slider,
-            click on the pencil that appears when you hover over the setting and
-            ignore that higher values are not recommended.
+            单个查询在整个集群上最多可使用的内存（单位：GB）。
+            建议值：worker 总内存的 50%~70%。
         </description>
+        <value-attributes>
+            <type>int</type>
+            <minimum>1</minimum>
+            <maximum>128</maximum>
+            <unit>GB</unit>
+            <increment-step>1</increment-step>
+            <overridable>false</overridable>
+        </value-attributes>
         <on-ambari-upgrade add="false"/>
     </property>
 
     <property>
-        <name>query.max-memory-per-node</name>
-        <value>1GB</value>
+        <name>query.max-memory-per-node-gb</name>
+        <display-name>Query Max Memory Per Node (GB)</display-name>
+        <value>1</value>
         <description>
-            The maximum amount of memory that a query may use on any one machine.
-
-            If you'd like to enter a value higher than the maximum on the slider,
-            click on the pencil that appears when you hover over the setting and
-            ignore that higher values are not recommended.
+            单个查询在任意一个 Worker 节点上可使用的最大内存（单位：GB）。
+            这是防止节点 OOM 的关键参数。
         </description>
+        <value-attributes>
+            <type>int</type>
+            <minimum>1</minimum>
+            <maximum>64</maximum>
+            <unit>GB</unit>
+            <increment-step>1</increment-step>
+            <overridable>false</overridable>
+        </value-attributes>
         <on-ambari-upgrade add="false"/>
     </property>
 
+
     <property>
         <name>spill-enabled</name>
         <value>false</value>
@@ -251,6 +264,65 @@
         </value-attributes>
         <on-ambari-upgrade add="false"/>
     </property>
+
+    <property>
+        <name>http-server.process-forwarded</name>
+        <value>true</value>
+        <description>
+            是否处理反向代理/负载均衡转发过来的请求头（Forwarded / X-Forwarded-*）。
+            开启后 Trino 会正确识别真实客户端 IP、原始 Host 以及协议（http/https），用于重定向与访问日志等场景。
+            适用于部署在 Nginx/Ingress/SLB 之后的场景；如果没有可信代理，建议关闭以避免伪造请求头带来的风险。
+        </description>
+        <value-attributes>
+            <type>boolean</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+
+    <property>
+        <name>web-ui.authentication.type</name>
+        <value>NONE</value>
+        <description>
+            Web UI 的认证方式（用于浏览器访问 Trino Web UI）。
+            常用可选值：FORM（表单登录）、FIXED（固定用户/免登录）、CERTIFICATE（证书）、KERBEROS（单点登录）、JWT、OAUTH2。
+            如果集群启用了 Kerberos 且希望浏览器 SSO，请选择 KERBEROS；生产环境不建议使用 NONE/免认证。
+        </description>
+        <value-attributes>
+            <type>value-list</type>
+            <entries>
+                <entry>
+                    <value>NONE</value>
+                    <label>NONE（免认证）</label>
+                </entry>
+                <entry>
+                    <value>FORM</value>
+                    <label>FORM（表单登录）</label>
+                </entry>
+                <entry>
+                    <value>FIXED</value>
+                    <label>FIXED（固定用户/免登录）</label>
+                </entry>
+                <entry>
+                    <value>CERTIFICATE</value>
+                    <label>CERTIFICATE（证书认证）</label>
+                </entry>
+                <entry>
+                    <value>KERBEROS</value>
+                    <label>KERBEROS（单点登录）</label>
+                </entry>
+                <entry>
+                    <value>JWT</value>
+                    <label>JWT</label>
+                </entry>
+                <entry>
+                    <value>OAUTH2</value>
+                    <label>OAUTH2</label>
+                </entry>
+            </entries>
+            <selection-cardinality>1</selection-cardinality>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
 
 
 </configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/alerts.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/alerts.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/alerts.json
new file mode 100644
--- /dev/null	(date 1765958702679)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/alerts.json	(date 1765958702679)
@@ -0,0 +1,32 @@
+{
+  "KNOX": {
+    "service": [],
+    "KNOX_GATEWAY": [
+      {
+        "name": "knox_gateway_process",
+        "label": "Knox Gateway Process",
+        "description": "This host-level alert is triggered if the Knox Gateway cannot be determined to be up.",
+        "interval": 1,
+        "scope": "HOST",
+        "source": {
+          "type": "PORT",
+          "uri": "{{gateway-site/gateway.port}}",
+          "default_port": 8443,
+          "reporting": {
+            "ok": {
+              "text": "TCP OK - {0:.3f}s response on port {1}"
+            },
+            "warning": {
+              "text": "TCP OK - {0:.3f}s response on port {1}",
+              "value": 1.5
+            },
+            "critical": {
+              "text": "Connection failed: {0} to {1}:{2}",
+              "value": 5.0
+            }
+          }
+        }
+      }
+    ]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/metainfo.xml
new file mode 100644
--- /dev/null	(date 1765958702684)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/metainfo.xml	(date 1765958702684)
@@ -0,0 +1,132 @@
+<?xml version="1.0"?>
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one or more
+   contributor license agreements.  See the NOTICE file distributed with
+   this work for additional information regarding copyright ownership.
+   The ASF licenses this file to You under the Apache License, Version 2.0
+   (the "License"); you may not use this file except in compliance with
+   the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+-->
+<metainfo>
+  <schemaVersion>2.0</schemaVersion>
+  <services>
+    <service>
+      <name>KNOX</name>
+      <displayName>Knox</displayName>
+      <comment>【增强】大数据网关与安全访问服务，用于为集群组件提供统一的 HTTP 访问入口与安全控制，使用前需安装并部署 Kerberos 及相关认证组件</comment>
+      <version>2.1.0</version>
+      <components>
+        <component>
+          <name>KNOX_GATEWAY</name>
+          <displayName>Knox Gateway</displayName>
+          <category>MASTER</category>
+          <cardinality>1+</cardinality>
+          <versionAdvertised>true</versionAdvertised>
+          <commandScript>
+            <script>scripts/knox_gateway.py</script>
+            <scriptType>PYTHON</scriptType>
+            <timeout>1200</timeout>
+          </commandScript>
+          <logs>
+            <log>
+              <logId>knox_gateway</logId>
+              <primary>true</primary>
+            </log>
+            <log>
+              <logId>knox_cli</logId>
+            </log>
+            <log>
+              <logId>knox_ldap</logId>
+            </log>
+          </logs>
+          <customCommands>
+            <customCommand>
+              <name>STARTDEMOLDAP</name>
+              <commandScript>
+                <script>scripts/knox_gateway.py</script>
+                <scriptType>PYTHON</scriptType>
+                <timeout>600</timeout>
+              </commandScript>
+            </customCommand>
+            <customCommand>
+              <name>STOPDEMOLDAP</name>
+              <commandScript>
+                <script>scripts/knox_gateway.py</script>
+                <scriptType>PYTHON</scriptType>
+                <timeout>600</timeout>
+              </commandScript>
+            </customCommand>
+          </customCommands>
+        </component>
+      </components>
+
+      <osSpecifics>
+        <osSpecific>
+          <osFamily>redhat9,redhat8,redhat7,amazonlinux2,redhat6,suse11,suse12,openeuler20,openeuler21,openeuler22,openeuler23,openeuler24,openeuler25,kylin10,kylin20,kylin30,uos10,uos20,anolis7,anolis8,anolis23</osFamily>
+          <packages>
+            <package>
+              <name>knox_${stack_version}</name>
+            </package>
+          </packages>
+        </osSpecific>
+        <osSpecific>
+          <osFamily>debian7,debian9,ubuntu12,ubuntu14,ubuntu16,ubuntu18,ubuntu20,ubuntu22,ubuntu24</osFamily>
+          <packages>
+            <package>
+              <name>knox-${stack_version}</name>
+            </package>
+          </packages>
+        </osSpecific>
+      </osSpecifics>
+
+      <commandScript>
+        <script>scripts/service_check.py</script>
+        <scriptType>PYTHON</scriptType>
+        <timeout>300</timeout>
+      </commandScript>
+
+      <configuration-dependencies>
+        <config-type>gateway-site</config-type>
+        <config-type>gateway-log4j</config-type>
+        <config-type>topology</config-type>
+        <config-type>admin-topology</config-type>
+        <config-type>knoxsso-topology</config-type>
+        <config-type>ranger-knox-plugin-properties</config-type>
+        <config-type>ranger-knox-audit</config-type>
+        <config-type>ranger-knox-policymgr-ssl</config-type>
+        <config-type>ranger-knox-security</config-type>
+        <config-type>gateway-log4j2</config-type>
+        <config-type>knoxcli-log4j2</config-type>
+        <config-type>ldap-log4j2</config-type>
+        <config-type>shell-log4j2</config-type>
+      </configuration-dependencies>
+
+      <themes>
+        <theme>
+          <fileName>credentials.json</fileName>
+          <default>true</default>
+        </theme>
+        <theme>
+          <fileName>directories.json</fileName>
+          <default>true</default>
+        </theme>
+      </themes>
+
+      <quickLinksConfigurations>
+        <quickLinksConfiguration>
+          <fileName>quicklinks.json</fileName>
+          <default>true</default>
+        </quickLinksConfiguration>
+      </quickLinksConfigurations>
+
+    </service>
+  </services>
+</metainfo>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZEPPELIN/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZEPPELIN/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZEPPELIN/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZEPPELIN/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZEPPELIN/metainfo.xml	(date 1765958707304)
@@ -21,10 +21,7 @@
     <service>
       <name>ZEPPELIN</name>
       <displayName>Zeppelin</displayName>
-      <comment>A web-based notebook that enables interactive data analytics. It enables you to
-        make beautiful data-driven, interactive and collaborative documents with SQL, Scala
-        and more.
-      </comment>
+      <comment>【增强】基于 Web 的交互式数据分析与开发环境，提供多语言 Notebook 能力用于数据探索与计算，使用前需安装并部署 Spark 等计算引擎及相关运行环境</comment>
       <version>0.10.1</version>
       <components>
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/service_advisor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/service_advisor.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/service_advisor.py
new file mode 100644
--- /dev/null	(date 1765958703041)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/service_advisor.py	(date 1765958703041)
@@ -0,0 +1,259 @@
+#!/usr/bin/env ambari-python-wrap
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+"""
+
+# Python imports
+import importlib.util
+import os
+import traceback
+import re
+import socket
+import fnmatch
+from lxml import etree                      
+import xml.etree.ElementTree as ET
+
+from resource_management.core.logger import Logger
+
+SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
+STACKS_DIR = os.path.join(SCRIPT_DIR, '../../../../../stacks/')
+PARENT_FILE = os.path.join(STACKS_DIR, 'service_advisor.py')
+
+try:
+  if "BASE_SERVICE_ADVISOR" in os.environ:
+    PARENT_FILE = os.environ["BASE_SERVICE_ADVISOR"]
+  with open(PARENT_FILE, 'rb') as fp:
+    spec = importlib.util.spec_from_file_location('service_advisor', PARENT_FILE)
+    service_advisor = importlib.util.module_from_spec(spec)
+    spec.loader.exec_module(service_advisor) 
+except Exception as e:
+  traceback.print_exc()
+  print("Failed to load parent")
+
+class KnoxServiceAdvisor(service_advisor.ServiceAdvisor):
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(KnoxServiceAdvisor, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    # Always call these methods
+    self.modifyMastersWithMultipleInstances()
+    self.modifyCardinalitiesDict()
+    self.modifyHeapSizeProperties()
+    self.modifyNotValuableComponents()
+    self.modifyComponentsNotPreferableOnServer()
+    self.modifyComponentLayoutSchemes()
+
+  def modifyMastersWithMultipleInstances(self):
+    """
+    Modify the set of masters with multiple instances.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyCardinalitiesDict(self):
+    """
+    Modify the dictionary of cardinalities.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyHeapSizeProperties(self):
+    """
+    Modify the dictionary of heap size properties.
+    Must be overriden in child class.
+    """
+    pass
+
+  def modifyNotValuableComponents(self):
+    """
+    Modify the set of components whose host assignment is based on other services.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentsNotPreferableOnServer(self):
+    """
+    Modify the set of components that are not preferable on the server.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentLayoutSchemes(self):
+    """
+    Modify layout scheme dictionaries for components.
+    The scheme dictionary basically maps the number of hosts to
+    host index where component should exist.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def getServiceComponentLayoutValidations(self, services, hosts):
+    """
+    Get a list of errors.
+    Must be overriden in child class.
+    """
+
+    return self.getServiceComponentCardinalityValidations(services, hosts, "KNOX")
+
+  def getServiceConfigurationRecommendations(self, configurations, clusterData, services, hosts):
+    """
+    Entry point.
+    Must be overriden in child class.
+    """
+    #Logger.info("Class: %s, Method: %s. Recommending Service Configurations." %
+    #            (self.__class__.__name__, inspect.stack()[0][3]))
+
+    recommender = KnoxRecommender()
+    recommender.recommendKnoxConfigurationsFromHDP22(configurations, clusterData, services, hosts)
+
+
+
+  def getServiceConfigurationsValidationItems(self, configurations, recommendedDefaults, services, hosts):
+    """
+    Entry point.
+    Validate configurations for the service. Return a list of errors.
+    The code for this function should be the same for each Service Advisor.
+    """
+    #Logger.info("Class: %s, Method: %s. Validating Configurations." %
+    #            (self.__class__.__name__, inspect.stack()[0][3]))
+
+    validator = KnoxValidator()
+    # Calls the methods of the validator using arguments,
+    # method(siteProperties, siteRecommendations, configurations, services, hosts)
+    return validator.validateListOfConfigUsingMethod(configurations, recommendedDefaults, services, hosts, validator.validators)
+
+
+
+class KnoxRecommender(service_advisor.ServiceAdvisor):
+  """
+  Knox Recommender suggests properties when adding the service for the first time or modifying configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(KnoxRecommender, self)
+    self.as_super.__init__(*args, **kwargs)
+
+
+  def recommendKnoxConfigurationsFromHDP22(self, configurations, clusterData, services, hosts):
+    if "ranger-env" in services["configurations"] and "ranger-knox-plugin-properties" in services["configurations"] and \
+        "ranger-knox-plugin-enabled" in services["configurations"]["ranger-env"]["properties"]:
+      putKnoxRangerPluginProperty = self.putProperty(configurations, "ranger-knox-plugin-properties", services)
+      rangerEnvKnoxPluginProperty = services["configurations"]["ranger-env"]["properties"]["ranger-knox-plugin-enabled"]
+      putKnoxRangerPluginProperty("ranger-knox-plugin-enabled", rangerEnvKnoxPluginProperty)
+
+    if 'topology' in services["configurations"] and 'content' in services["configurations"]["topology"]["properties"]:
+      putKnoxTopologyContent = self.putProperty(configurations, "topology", services)
+      rangerPluginEnabled = ''
+      if 'ranger-knox-plugin-properties' in configurations and 'ranger-knox-plugin-enabled' in  configurations['ranger-knox-plugin-properties']['properties']:
+        rangerPluginEnabled = configurations['ranger-knox-plugin-properties']['properties']['ranger-knox-plugin-enabled']
+      elif 'ranger-knox-plugin-properties' in services['configurations'] and 'ranger-knox-plugin-enabled' in services['configurations']['ranger-knox-plugin-properties']['properties']:
+        rangerPluginEnabled = services['configurations']['ranger-knox-plugin-properties']['properties']['ranger-knox-plugin-enabled']
+
+      # check if authorization provider already added
+      topologyContent = services["configurations"]["topology"]["properties"]["content"]
+      authorizationProviderExists = False
+      authNameChanged = False
+      parser = etree.XMLParser(recover=True)
+      root = etree.fromstring(topologyContent, parser=parser)
+      if root is not None:
+        gateway = root.find("gateway")
+        if gateway is not None:
+          for provider in gateway.findall('provider'):
+            role = provider.find('role')
+            if role is not None and role.text and role.text.lower() == "authorization":
+              authorizationProviderExists = True
+
+            name = provider.find('name')
+            if name is not None and name.text == "AclsAuthz" and rangerPluginEnabled \
+               and rangerPluginEnabled.lower() == "Yes".lower():
+              newAuthName = "XASecurePDPKnox"
+              authNameChanged = True
+            elif name is not None and (((not rangerPluginEnabled) or rangerPluginEnabled.lower() != "Yes".lower()) \
+               and name.text == 'XASecurePDPKnox'):
+              newAuthName = "AclsAuthz"
+              authNameChanged = True
+
+            if authNameChanged:
+              name.text = newAuthName
+              import xml.etree.ElementTree as ET                                  
+              putKnoxTopologyContent('content', ET.tostring(root, encoding='unicode'))
+
+            if authorizationProviderExists:
+              break
+
+      if not authorizationProviderExists:
+        if root is not None:
+          gateway = root.find("gateway")
+          if gateway is not None:
+            provider = ET.SubElement(gateway, 'provider')
+
+            role = ET.SubElement(provider, 'role')
+            role.text = "authorization"
+
+            name = ET.SubElement(provider, 'name')
+            if rangerPluginEnabled and rangerPluginEnabled.lower() == "Yes".lower():
+              name.text = "XASecurePDPKnox"
+            else:
+              name.text = "AclsAuthz"
+
+            enabled = ET.SubElement(provider, 'enabled')
+            enabled.text = "true"
+
+            #TODO add pretty format for newly added provider
+            putKnoxTopologyContent('content', ET.tostring(root, encoding='unicode'))
+
+
+
+
+class KnoxValidator(service_advisor.ServiceAdvisor):
+  """
+  Knox Validator checks the correctness of properties whenever the service is first added or the user attempts to
+  change configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(KnoxValidator, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    self.validators = [("ranger-knox-plugin-properties", self.validateKnoxRangerPluginConfigurationsFromHDP22),
+                       ]
+
+  def validateKnoxRangerPluginConfigurationsFromHDP22(self, properties, recommendedDefaults, configurations, services, hosts):
+    validationItems = []
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    ranger_plugin_properties = self.getSiteProperties(configurations, "ranger-knox-plugin-properties")
+    ranger_plugin_enabled = ranger_plugin_properties['ranger-knox-plugin-enabled'] if ranger_plugin_properties else 'No'
+    if 'RANGER' in servicesList and ranger_plugin_enabled.lower() == 'yes':
+      # ranger-hdfs-plugin must be enabled in ranger-env
+      ranger_env = self.getServicesSiteProperties(services, 'ranger-env')
+      if not ranger_env or not 'ranger-knox-plugin-enabled' in ranger_env or \
+          ranger_env['ranger-knox-plugin-enabled'].lower() != 'yes':
+        validationItems.append({"config-name": 'ranger-knox-plugin-enabled',
+                                "item": self.getWarnItem(
+                                  "ranger-knox-plugin-properties/ranger-knox-plugin-enabled must correspond ranger-env/ranger-knox-plugin-enabled")})
+    return self.toConfigurationValidationProblems(validationItems, "ranger-knox-plugin-properties")
+
+
+
+
+
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/kerberos.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/kerberos.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/kerberos.json
new file mode 100644
--- /dev/null	(date 1765958703037)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/kerberos.json	(date 1765958703037)
@@ -0,0 +1,70 @@
+{
+  "services": [
+    {
+      "name": "KNOX",
+      "components": [
+        {
+          "name": "KNOX_GATEWAY",
+          "identities": [
+            {
+              "name": "knox_principal",
+              "principal": {
+                "value": "${knox-env/knox_user}/_HOST@${realm}",
+                "type" : "service",
+                "configuration": "knox-env/knox_principal_name",
+                "local_username": "${knox-env/knox_user}"
+
+              },
+              "keytab": {
+                "file": "${keytab_dir}/knox.service.keytab",
+                "owner": {
+                  "name": "${knox-env/knox_user}",
+                  "access": "r"
+                },
+                "group": {
+                  "name": "${cluster-env/user_group}",
+                  "access": ""
+                },
+                "configuration": "knox-env/knox_keytab_path"
+              }
+            },
+            {
+              "name": "knox_knox_gateway_knox_principal",
+              "reference": "/KNOX/KNOX_GATEWAY/knox_principal",
+              "principal": {
+                "configuration": "ranger-knox-audit/xasecure.audit.jaas.Client.option.principal"                
+              },
+              "keytab": {
+                "configuration": "ranger-knox-audit/xasecure.audit.jaas.Client.option.keyTab"
+              }
+            }
+          ],
+          "configurations": [
+            {
+              "gateway-site": {
+                "gateway.hadoop.kerberos.secured": "true",
+                "java.security.krb5.conf": "/etc/krb5.conf"
+              }
+            },
+            {
+              "core-site": {
+                "hadoop.proxyuser.${knox-env/knox_user}.groups": "${hadoop-env/proxyuser_group}",
+                "hadoop.proxyuser.${knox-env/knox_user}.hosts": "${clusterHostInfo/knox_gateway_hosts}"
+              }
+            },
+            {
+              "ranger-knox-audit": {
+                "xasecure.audit.jaas.Client.loginModuleName": "com.sun.security.auth.module.Krb5LoginModule",
+                "xasecure.audit.jaas.Client.loginModuleControlFlag": "required",
+                "xasecure.audit.jaas.Client.option.useKeyTab": "true",
+                "xasecure.audit.jaas.Client.option.storeKey": "false",
+                "xasecure.audit.jaas.Client.option.serviceName": "solr",
+                "xasecure.audit.destination.solr.force.use.inmemory.jaas.config": "true"
+              }
+            }
+          ]
+        }
+      ]
+    }
+  ]
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/themes/directories.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/themes/directories.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/themes/directories.json
new file mode 100644
--- /dev/null	(date 1765958702277)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/themes/directories.json	(date 1765958702277)
@@ -0,0 +1,95 @@
+{
+  "name": "directories",
+  "description": "Directories theme for HIVE service",
+  "configuration": {
+    "layouts": [
+      {
+        "name": "directories",
+        "tabs": [
+          {
+            "name": "directories",
+            "display-name": "Directories",
+            "layout": {
+              "tab-columns": "1",
+              "tab-rows": "4",
+              "sections": [
+                {
+                  "name": "subsection-data-dirs",
+                  "display-name": "DATA DIRS",
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-data-dirs",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                },
+                {
+                  "name": "subsection-pid-dirs",
+                  "display-name": "PID DIRS",
+                  "row-index": "2",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-pid-dirs",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                }
+              ]
+            }
+          }
+        ]
+      }
+    ],
+    "placement": {
+      "configuration-layout": "default",
+      "configs": [
+        {
+          "config": "gateway-site/gateway.gateway.conf.dir",
+          "subsection-name": "subsection-data-dirs"
+        },
+        {
+          "config": "knox-env/knox_pid_dir",
+          "subsection-name": "subsection-pid-dirs"
+        }
+      ]
+    },
+    "widgets": [
+      {
+        "config": "gateway-site/gateway.gateway.conf.dir",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "knox-env/knox_pid_dir",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "gateway-site/gateway.httpclient.truststore.type",
+        "widget": {
+          "type": "combo"
+        }
+      }
+
+    ]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/themes/credentials.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/themes/credentials.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/themes/credentials.json
new file mode 100644
--- /dev/null	(date 1765958702271)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/themes/credentials.json	(date 1765958702271)
@@ -0,0 +1,38 @@
+{
+  "name": "credentials",
+  "configuration": {
+    "placement": {
+      "configs": [
+        {
+          "config": "knox-env/knox_master_secret",
+          "subsection-name": "subsection-knox-master-secret"
+        }
+      ],
+      "configuration-layout": "credentials"
+    },
+    "widgets": [],
+    "layouts": [
+      {
+        "name": "credentials",
+        "tabs": [
+          {
+            "name": "credentials",
+            "layout": {
+              "sections": [
+                {
+                  "subsections": [
+                    {
+                      "name": "subsection-knox-master-secret",
+                      "display-name": "Knox Master Secret"
+                    }
+                  ],
+                  "name": "credentials"
+                }
+              ]
+            }
+          }
+        ]
+      }
+    ]
+  }
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI-METRICS/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI-METRICS/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI-METRICS/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI-METRICS/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI-METRICS/metainfo.xml	(date 1765958707681)
@@ -21,6 +21,7 @@
     <service>
       <name>AMBARI_METRICS</name>
       <extends>common-services/AMBARI_METRICS/3.0.0</extends>
+      <comment>【增强】提供集群与服务组件的指标监控能力，使用前需安装并部署 Solr、ZooKeeper、HBase 等依赖组件</comment>
     </service>
   </services>
 </metainfo>
\ No newline at end of file
Index: ambari-web/app/messages.js
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-web/app/messages.js b/ambari-web/app/messages.js
--- a/ambari-web/app/messages.js	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-web/app/messages.js	(date 1765940367370)
@@ -903,9 +903,9 @@
   'installer.step3.selectedHosts.popup.header':'Selected Hosts',
 
   'installer.step4.header':'Choose Services',
-  'installer.step4.body':'Choose which services you want to install on your cluster.',
+  'installer.step4.body':'<strong>安装过程指导：</strong><a href="https://doc.janettr.com/install/component" target="_blank" rel="noopener">https://doc.janettr.com/install/component</a>',
   'installer.step4.headerFS':'Choose File System',
-  'installer.step4.bodyFS':'Choose which file system you want to install on your cluster.',
+  'installer.step4.bodyFS':'<strong>提示：</strong>请仔细阅读<a href="https://doc.janettr.com/update/" target="_blank" rel="noopener">更新说明</a>。首页有交流群，可反馈 Bug、分享使用经验并提出建议。',
   'installer.step4.fsCheck.popup.header':'File System Required',
   'installer.step4.fsCheck.popup.body':'You did not select a File System but one is required. We will automatically add {0}. Is this OK?',
   'installer.step4.multipleDFS.popup.header':'Multiple File Systems Selected',
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/files/validateKnoxStatus.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/files/validateKnoxStatus.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/files/validateKnoxStatus.py
new file mode 100644
--- /dev/null	(date 1765958702281)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/files/validateKnoxStatus.py	(date 1765958702281)
@@ -0,0 +1,43 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+import optparse
+import socket
+
+#
+# Main.
+#
+def main():
+  parser = optparse.OptionParser(usage="usage: %prog [options]")
+  parser.add_option("-p", "--port", dest="port", help="Port for Knox process")
+  parser.add_option("-n", "--hostname", dest="hostname", help="Hostname of Knox Gateway component")
+
+  (options, args) = parser.parse_args()
+  timeout_seconds = 5
+  try:
+    s = socket.create_connection((options.hostname, int(options.port)),timeout=timeout_seconds)
+    print("Successfully connected to %s on port %s" % (options.hostname, options.port))
+    s.close()
+  except socket.error as e:
+    print("Connection to %s on port %s failed: %s" % (options.hostname, options.port, e))
+    exit(1)
+
+if __name__ == "__main__":
+  main()
+
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/scripts/params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/scripts/params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/scripts/params.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/scripts/params.py	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/scripts/params.py	(date 1765958706011)
@@ -122,7 +122,7 @@
 
 trino_log_properties_content = config["configurations"]["log-properties"]["content"]
 
-memory_configs = ["query.max-memory-per-node", "query.max-memory"]
+memory_configs = ["query.max-memory-per-node-gb", "query.max-memory-gb"]
 
 # jdk_home
 trino_jdk_home = config["configurations"]["trino-env"]["trino_jdk_home"]
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/OZONE/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/OZONE/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/OZONE/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/OZONE/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/OZONE/metainfo.xml	(date 1765958705064)
@@ -21,7 +21,7 @@
         <service>
             <name>OZONE</name>
             <displayName>Ozone</displayName>
-            <comment>Component Ozone Integrated By JaneTTR . For commercial use, please contact mail: 3832514048@qq.com</comment>
+            <comment>【增强】面向对象存储场景的分布式存储服务，提供高可靠、高可扩展的数据存储能力，使用前需安装并部署必要的存储与元数据组件</comment>
             <version>1.4.1</version>
             <selection>TECH_PREVIEW</selection>
 			<deleted>true</deleted> 
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox_gateway.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox_gateway.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox_gateway.py
new file mode 100644
--- /dev/null	(date 1765958702627)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox_gateway.py	(date 1765958702627)
@@ -0,0 +1,216 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+import os
+
+from resource_management.libraries.script.script import Script
+from resource_management.libraries.functions.check_process_status import check_process_status
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions import stack_select
+from resource_management.libraries.functions.constants import Direction
+from resource_management.libraries.functions.security_commons import build_expectations
+from resource_management.libraries.functions.security_commons import cached_kinit_executor
+from resource_management.libraries.functions.security_commons import validate_security_config_properties
+from resource_management.libraries.functions.security_commons import get_params_from_filesystem
+from resource_management.libraries.functions.security_commons import FILE_TYPE_XML
+from resource_management.libraries.functions.show_logs import show_logs
+from resource_management.core.resources.system import File, Execute, Link
+from resource_management.core.resources.service import Service
+from resource_management.core.logger import Logger
+
+from ambari_commons import OSConst, OSCheck
+from ambari_commons.os_family_impl import OsFamilyImpl
+
+if OSCheck.is_windows_family():
+    from resource_management.libraries.functions.windows_service_utils import check_windows_service_status
+
+import upgrade
+from knox import knox, update_knox_logfolder_permissions
+from knox_ldap import ldap
+from setup_ranger_knox import setup_ranger_knox
+
+
+class KnoxGateway(Script):
+    def install(self, env):
+        import params
+        env.set_params(params)
+        self.install_packages(env)
+
+        File(os.path.join(params.knox_conf_dir, 'topologies', 'sandbox.xml'),
+             action="delete",
+             )
+
+    def configure(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        knox()
+        ldap()
+
+    def configureldap(self, env):
+        import params
+        env.set_params(params)
+        ldap()
+
+
+@OsFamilyImpl(os_family=OSConst.WINSRV_FAMILY)
+class KnoxGatewayWindows(KnoxGateway):
+    def start(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        self.configure(env)
+        # setup_ranger_knox(env)
+        Service(params.knox_gateway_win_service_name, action="start")
+
+    def stop(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        Service(params.knox_gateway_win_service_name, action="stop")
+
+    def status(self, env):
+        import status_params
+        env.set_params(status_params)
+        check_windows_service_status(status_params.knox_gateway_win_service_name)
+
+    def startdemoldap(self, env):
+        import params
+        env.set_params(params)
+        self.configureldap(env)
+        Service(params.knox_ldap_win_service_name, action="start")
+
+    def stopdemoldap(self, env):
+        import params
+        env.set_params(params)
+        Service(params.knox_ldap_win_service_name, action="stop")
+
+
+@OsFamilyImpl(os_family=OsFamilyImpl.DEFAULT)
+class KnoxGatewayDefault(KnoxGateway):
+
+    def pre_upgrade_restart(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+
+        # backup the data directory to /tmp/knox-upgrade-backup/knox-data-backup.tar just in case
+        # something happens; Knox is interesting in that they re-generate missing files like
+        # keystores which can cause side effects if the upgrade goes wrong
+        if params.upgrade_direction and params.upgrade_direction == Direction.UPGRADE:
+            absolute_backup_dir = upgrade.backup_data()
+            Logger.info("Knox data was successfully backed up to {0}".format(absolute_backup_dir))
+
+        stack_select.select_packages(params.version)
+
+        # seed the new Knox data directory with the keystores of yesteryear
+        if params.upgrade_direction == Direction.UPGRADE:
+            upgrade.seed_current_data_directory()
+
+    def start(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        self.configure(env)
+        daemon_cmd = format('{knox_bin} start')
+        no_op_test = format('ls {knox_pid_file} >/dev/null 2>&1 && ps -p `cat {knox_pid_file}` >/dev/null 2>&1')
+        setup_ranger_knox(upgrade_type=upgrade_type)
+        # Used to setup symlink, needed to update the knox managed symlink, in case of custom locations
+        if os.path.islink(params.knox_managed_pid_symlink):
+            Link(params.knox_managed_pid_symlink,
+                 to=params.knox_pid_dir,
+                 )
+
+        update_knox_logfolder_permissions()
+
+        try:
+            Execute(daemon_cmd,
+                    user=params.knox_user,
+                    environment={
+                        'JAVA_HOME': params.java_home,
+                        'HADOOP_CONF_DIR': '/etc/hadoop/conf',
+                        'APP_HOME_DIR': '/usr/bigtop/current/knox-server'
+                    },
+                    not_if=no_op_test
+                    )
+        except:
+            show_logs(params.knox_logs_dir, params.knox_user)
+            raise
+
+    def stop(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        daemon_cmd = format('{knox_bin} stop')
+
+        update_knox_logfolder_permissions()
+
+        try:
+            Execute(daemon_cmd,
+                    environment={'JAVA_HOME': params.java_home},
+                    user=params.knox_user,
+                    )
+        except:
+            show_logs(params.knox_logs_dir, params.knox_user)
+            raise
+
+        File(params.knox_pid_file,
+             action="delete",
+             )
+
+    def status(self, env):
+        import status_params
+        env.set_params(status_params)
+        check_process_status(status_params.knox_pid_file)
+
+    def startdemoldap(self, env):
+        import params
+        env.set_params(params)
+        self.configureldap(env)
+        daemon_cmd = format('{ldap_bin} start')
+        no_op_test = format('ls {ldap_pid_file} >/dev/null 2>&1 && ps -p `cat {ldap_pid_file}` >/dev/null 2>&1')
+        Execute(daemon_cmd,
+                user=params.knox_user,
+                environment={'JAVA_HOME': params.java_home},
+                not_if=no_op_test
+                )
+
+    def stopdemoldap(self, env):
+        import params
+        env.set_params(params)
+        self.configureldap(env)
+        daemon_cmd = format('{ldap_bin} stop')
+        Execute(daemon_cmd,
+                environment={'JAVA_HOME': params.java_home},
+                user=params.knox_user,
+                )
+        File(params.ldap_pid_file,
+             action="delete"
+             )
+
+    def get_log_folder(self):
+        import params
+        return params.knox_logs_dir
+
+    def get_user(self):
+        import params
+        return params.knox_user
+
+    def get_pid_files(self):
+        import status_params
+        return [status_params.knox_pid_file]
+
+
+if __name__ == "__main__":
+    KnoxGateway().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/VICTORIAMETRICS/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/VICTORIAMETRICS/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/VICTORIAMETRICS/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/VICTORIAMETRICS/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/VICTORIAMETRICS/metainfo.xml	(date 1765958707704)
@@ -5,7 +5,7 @@
         <service>
             <name>VICTORIAMETRICS</name>
             <displayName>Victoriametrics</displayName>
-            <comment>Component VICTORIAMETRICS Integrated By JaneTTR . For commercial use, please contact mail: 3832514048@qq.com</comment>
+            <comment>【废弃】临时废弃，待思考完毕，后续可能将以全新姿势与大家见面！</comment>
             <version>1.109.1</version>
             <components>
                 <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params_linux.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params_linux.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params_linux.py
new file mode 100644
--- /dev/null	(date 1765958702634)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params_linux.py	(date 1765958702634)
@@ -0,0 +1,650 @@
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+Ambari Agent
+
+"""
+import os
+import status_params
+
+from resource_management.core.logger import Logger
+
+import \
+    ambari_simplejson as json  # simplejson is much faster comparing to Python 2.6 json module and has the same functions set.
+from resource_management.libraries.functions import format
+from resource_management.libraries.functions.version import format_stack_version
+from resource_management.libraries.functions.default import default
+from resource_management.libraries.functions.get_port_from_url import get_port_from_url
+from resource_management.libraries.functions.get_stack_version import get_stack_version
+from resource_management.libraries.functions import get_kinit_path
+from resource_management.libraries.script.script import Script
+from status_params import *
+from resource_management.libraries.resources.hdfs_resource import HdfsResource
+from resource_management.libraries.functions import stack_select, conf_select
+from resource_management.libraries.functions.get_not_managed_resources import get_not_managed_resources
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions.stack_features import get_stack_feature_version
+from resource_management.libraries.functions import upgrade_summary
+from resource_management.libraries.functions.constants import StackFeature
+from resource_management.libraries.functions import is_empty
+from resource_management.libraries.functions.setup_ranger_plugin_xml import get_audit_configs, \
+    generate_ranger_service_config
+
+# server configurations
+config = Script.get_config()
+stack_root = Script.get_stack_root()
+
+tmp_dir = Script.get_tmp_dir()
+stack_name = status_params.stack_name
+upgrade_direction = default("/commandParams/upgrade_direction", None)
+version = default("/commandParams/version", None)
+# E.g., 2.3.2.0
+version_formatted = format_stack_version(version)
+
+# E.g., 2.3
+stack_version_unformatted = config['clusterLevelParams']['stack_version']
+stack_version_formatted = format_stack_version(stack_version_unformatted)
+
+# get the correct version to use for checking stack features
+version_for_stack_feature_checks = get_stack_feature_version(config)
+
+stack_supports_ranger_kerberos = check_stack_feature(StackFeature.RANGER_KERBEROS_SUPPORT,
+                                                     version_for_stack_feature_checks)
+stack_supports_ranger_audit_db = check_stack_feature(StackFeature.RANGER_AUDIT_DB_SUPPORT,
+                                                     version_for_stack_feature_checks)
+stack_supports_core_site_for_ranger_plugin = check_stack_feature(StackFeature.CORE_SITE_FOR_RANGER_PLUGINS_SUPPORT,
+                                                                 version_for_stack_feature_checks)
+
+# This is the version whose state is CURRENT. During an RU, this is the source version.
+# DO NOT format it since we need the build number too.
+upgrade_from_version = upgrade_summary.get_source_version()
+
+# server configurations
+# Default value used in HDP 2.3.0.0 and earlier.
+knox_data_dir = '/var/lib/knox/data'
+
+# Important, it has to be strictly greater than 2.3.0.0!!!
+Logger.info(format("Stack version to use is {version_formatted}"))
+if version_formatted and check_stack_feature(StackFeature.KNOX_VERSIONED_DATA_DIR, version_formatted):
+    # This is the current version. In the case of a Rolling Upgrade, it will be the newer version.
+    # In the case of a Downgrade, it will be the version downgrading to.
+    # This is always going to be a symlink to /var/lib/knox/data_${version}
+    knox_data_dir = format('{stack_root}/{version}/knox/data')
+    Logger.info(format("Detected stack with version {version}, will use knox_data_dir = {knox_data_dir}"))
+
+knox_master_secret_path = format('{knox_data_dir}/security/master')
+knox_cert_store_path = format('{knox_data_dir}/security/keystores/gateway.jks')
+knox_user = default("/configurations/knox-env/knox_user", "knox")
+
+# server configurations
+knox_data_dir = '/var/lib/knox/data'
+knox_logs_dir = '/var/log/knox'
+
+# default parameters
+knox_bin = '/usr/bin/gateway'
+knox_conf_dir = '/etc/knox/conf'
+ldap_bin = '/usr/lib/knox/bin/ldap.sh'
+knox_client_bin = '/usr/lib/knox/bin/knoxcli.sh'
+
+# HDP 2.2+ parameters
+if stack_version_formatted and check_stack_feature(StackFeature.ROLLING_UPGRADE, stack_version_formatted):
+    knox_bin = format('{stack_root}/current/knox-server/bin/gateway.sh')
+    knox_conf_dir = format('{stack_root}/current/knox-server/conf')
+    ldap_bin = format('{stack_root}/current/knox-server/bin/ldap.sh')
+    knox_client_bin = format('{stack_root}/current/knox-server/bin/knoxcli.sh')
+    knox_master_secret_path = format('{stack_root}/current/knox-server/data/security/master')
+    knox_cert_store_path = format('{stack_root}/current/knox-server/data/security/keystores/gateway.jks')
+    knox_data_dir = format('{stack_root}/current/knox-server/data/')
+
+knox_group = default("/configurations/knox-env/knox_group", "hadoop")
+mode = 0o644
+
+stack_version_unformatted = config['clusterLevelParams']['stack_version']
+stack_version_formatted = format_stack_version(stack_version_unformatted)
+
+namenode_hosts = default("/clusterHostInfo/namenode_hosts", None)
+has_namenode = bool(namenode_hosts)
+
+dfs_ha_enabled = False
+dfs_ha_nameservices = default('/configurations/hdfs-site/dfs.internal.nameservices', None)
+if dfs_ha_nameservices is None:
+    dfs_ha_nameservices = default('/configurations/hdfs-site/dfs.nameservices', None)
+
+if dfs_ha_nameservices is not None:
+    dfs_ha_nameservices = dfs_ha_nameservices.split(',')[
+        0]  # for now knox topology.xml supports working with only one nameservice
+
+dfs_ha_namenode_ids = default(format("/configurations/hdfs-site/dfs.ha.namenodes.{dfs_ha_nameservices}"), None)
+
+namenode_rpc = None
+
+dfs_type = default("/clusterLevelParams/dfs_type", "").lower()
+
+namenode_http_port = "50070"
+namenode_https_port = "50470"
+namenode_rpc_port = "8020"
+namenode_address = ""
+hdfs_ui_url = ""
+hdfs_ui_url_1 = ""
+hdfs_ui_url_2 = ""
+
+if has_namenode:
+    if 'dfs.namenode.http-address' in config['configurations']['hdfs-site']:
+        namenode_http_port = get_port_from_url(config['configurations']['hdfs-site']['dfs.namenode.http-address'])
+    if 'dfs.namenode.https-address' in config['configurations']['hdfs-site']:
+        namenode_https_port = get_port_from_url(config['configurations']['hdfs-site']['dfs.namenode.https-address'])
+    if dfs_ha_enabled and namenode_rpc:
+        namenode_rpc_port = get_port_from_url(namenode_rpc)
+    else:
+        if 'dfs.namenode.rpc-address' in config['configurations']['hdfs-site']:
+            namenode_rpc_port = get_port_from_url(config['configurations']['hdfs-site']['dfs.namenode.rpc-address'])
+
+    namenode_address = format("{dfs_type}://{namenode_hosts[0]}:{namenode_rpc_port}")
+    hdfs_ui_url = format("http://{namenode_hosts[0]}:{namenode_http_port}")
+    hdfs_ui_url_1 = hdfs_ui_url
+
+if dfs_ha_namenode_ids:
+    dfs_ha_namemodes_ids_list = dfs_ha_namenode_ids.split(",")
+    dfs_ha_namenode_ids_array_len = len(dfs_ha_namemodes_ids_list)
+    if dfs_ha_namenode_ids_array_len > 1:
+        dfs_ha_enabled = True
+
+if dfs_ha_enabled:
+    for nn_id in dfs_ha_namemodes_ids_list:
+        nn_host = config['configurations']['hdfs-site'][
+            format('dfs.namenode.rpc-address.{dfs_ha_nameservices}.{nn_id}')]
+        if hostname.lower() in nn_host.lower():
+            namenode_id = nn_id
+            namenode_rpc = nn_host
+        # With HA enabled namenode_address is recomputed
+    namenode_address = format('{dfs_type}://{dfs_ha_nameservices}')
+    hdfs_ui_url_2 = format("http://{namenode_hosts[1]}:{namenode_http_port}")
+
+namenode_port_map = {}
+if dfs_ha_enabled:
+    for nn_id in dfs_ha_namemodes_ids_list:
+        nn_host = config['configurations']['hdfs-site'][
+            format('dfs.namenode.http-address.{dfs_ha_nameservices}.{nn_id}')]
+        nn_host_parts = nn_host.split(':')
+        namenode_port_map[nn_host_parts[0]] = nn_host_parts[1]
+
+dfs_http_policy = default('/configurations/hdfs-site/dfs.http.policy', None)
+
+hdfs_https_on = False
+hdfs_scheme = 'http'
+if dfs_http_policy != None:
+    hdfs_https_on = (dfs_http_policy.upper() == 'HTTPS_ONLY')
+    hdfs_scheme = 'http' if not hdfs_https_on else 'https'
+    hdfs_port = str(namenode_http_port) if not hdfs_https_on else str(namenode_https_port)
+    namenode_http_port = hdfs_port
+
+
+def buildUrlElement(protocol, hdfs_host, port, servicePath):
+    openTag = "<url>"
+    closeTag = "</url>"
+    proto = protocol + "://"
+    newLine = "\n"
+    if hdfs_host is None or port is None:
+        return ""
+    else:
+        return openTag + proto + hdfs_host + ":" + port + servicePath + closeTag + newLine
+
+
+namenode_host_keys = namenode_port_map.keys();
+webhdfs_service_urls = ""
+if len(namenode_host_keys) > 0:
+    for host in namenode_host_keys:
+        webhdfs_service_urls += buildUrlElement("http", host, namenode_port_map[host], "/webhdfs")
+elif has_namenode:
+    webhdfs_service_urls = buildUrlElement("http", namenode_hosts[0], namenode_http_port, "/webhdfs")
+
+yarn_http_policy = default('/configurations/yarn-site/yarn.http.policy', None)
+yarn_https_on = False
+yarn_scheme = 'http'
+if yarn_http_policy != None:
+    yarn_https_on = (yarn_http_policy.upper() == 'HTTPS_ONLY')
+    yarn_scheme = 'http' if not yarn_https_on else 'https'
+
+rm_hosts = default("/clusterHostInfo/resourcemanager_hosts", None)
+if type(rm_hosts) is list:
+    rm_host = rm_hosts[0]
+else:
+    rm_host = rm_hosts
+has_rm = not rm_host == None
+
+jt_rpc_port = "8050"
+rm_port = "8080"
+
+if has_rm:
+    if 'yarn.resourcemanager.address' in config['configurations']['yarn-site']:
+        jt_rpc_port = get_port_from_url(config['configurations']['yarn-site']['yarn.resourcemanager.address'])
+
+    if 'yarn.resourcemanager.webapp.address' in config['configurations']['yarn-site']:
+        rm_port = get_port_from_url(config['configurations']['yarn-site']['yarn.resourcemanager.webapp.address'])
+
+hive_http_port = default('/configurations/hive-site/hive.server2.thrift.http.port', "10001")
+hive_http_path = default('/configurations/hive-site/hive.server2.thrift.http.path', "cliservice")
+hive_server_hosts = default("/clusterHostInfo/hive_server_hosts", None)
+if type(hive_server_hosts) is list:
+    hive_server_host = hive_server_hosts[0] if len(hive_server_hosts) > 0 else None
+else:
+    hive_server_host = hive_server_hosts
+
+templeton_port = default('/configurations/webhcat-site/templeton.port', "50111")
+webhcat_server_hosts = default("/clusterHostInfo/webhcat_server_hosts", None)
+if type(webhcat_server_hosts) is list:
+    webhcat_server_host = webhcat_server_hosts[0]
+else:
+    webhcat_server_host = webhcat_server_hosts
+
+hive_scheme = 'http'
+webhcat_scheme = 'http'
+
+hbase_master_scheme = 'http'
+hbase_master_ui_port = default('/configurations/hbase-site/hbase.master.info.port', "16010");
+hbase_master_port = default('/configurations/hbase-site/hbase.rest.port', "8080")
+is_hbase_installed = default("/clusterHostInfo/hbase_master_hosts", None) is not None
+hbase_master_hosts = default("/clusterHostInfo/hbase_master_hosts", None)
+if is_hbase_installed and type(hbase_master_hosts) is list:
+    hbase_master_host = hbase_master_hosts[0]
+else:
+    hbase_master_host = hbase_master_hosts
+
+#
+# Oozie
+#
+oozie_https_port = None
+oozie_scheme = 'http'
+oozie_server_port = "11000"
+oozie_server_hosts = default("/clusterHostInfo/oozie_server_hosts", None)
+
+if type(oozie_server_hosts) is list:
+    oozie_server_host = oozie_server_hosts[0]
+else:
+    oozie_server_host = oozie_server_hosts
+
+has_oozie = not oozie_server_host == None
+
+if has_oozie:
+    oozie_server_port = get_port_from_url(config['configurations']['oozie-site']['oozie.base.url'])
+    oozie_https_port = default("/configurations/oozie-site/oozie.https.port", None)
+
+if oozie_https_port is not None:
+    oozie_scheme = 'https'
+    oozie_server_port = oozie_https_port
+
+#
+# Falcon
+#
+falcon_server_hosts = default("/clusterHostInfo/falcon_server_hosts", None)
+if type(falcon_server_hosts) is list:
+    falcon_server_host = falcon_server_hosts[0]
+else:
+    falcon_server_host = falcon_server_hosts
+
+falcon_scheme = 'http'
+has_falcon = not falcon_server_host == None
+falcon_server_port = "15000"
+
+if has_falcon:
+    falcon_server_port = config['configurations']['falcon-env']['falcon_port']
+
+#
+# JobHistory mapreduce
+#
+mr_scheme = 'http'
+mr_historyserver_address = default("/configurations/mapred-site/mapreduce.jobhistory.webapp.address", None)
+
+#
+# Yarn nodemanager
+#
+nodeui_scheme = 'http'
+nodeui_port = "8042"
+nm_hosts = default("/clusterHostInfo/nodemanager_hosts", None)
+if type(nm_hosts) is list:
+    nm_host = nm_hosts[0]
+else:
+    nm_host = nm_hosts
+
+has_yarn = default("/configurations/yarn-site", None)
+if has_yarn and 'yarn.nodemanager.webapp.address' in config['configurations']['yarn-site']:
+    nodeui_port = get_port_from_url(config['configurations']['yarn-site']['yarn.nodemanager.webapp.address'])
+
+#
+# Spark Thrift UI
+#
+spark_thriftserver_scheme = 'http'
+spark_thriftserver_ui_port = 4039
+spark_thriftserver_hosts = default("/clusterHostInfo/spark_thriftserver_hosts", None)
+if type(spark_thriftserver_hosts) is list:
+    spark_thriftserver_host = spark_thriftserver_hosts[0]
+else:
+    spark_thriftserver_host = spark_thriftserver_hosts
+
+# Knox managed properties
+knox_managed_pid_symlink = format('{stack_root}/current/knox-server/pids')
+
+# knox log4j
+knox_gateway_log_maxfilesize = default('/configurations/gateway-log4j/knox_gateway_log_maxfilesize', 256)
+knox_gateway_log_maxbackupindex = default('/configurations/gateway-log4j/knox_gateway_log_maxbackupindex', 20)
+knox_ldap_log_maxfilesize = default('/configurations/ldap-log4j/knox_ldap_log_maxfilesize', 256)
+knox_ldap_log_maxbackupindex = default('/configurations/ldap-log4j/knox_ldap_log_maxbackupindex', 20)
+
+# server configurations
+knox_master_secret = config['configurations']['knox-env']['knox_master_secret']
+knox_host_name = config['clusterHostInfo']['knox_gateway_hosts'][0]
+knox_host_name_in_cluster = config['agentLevelParams']['hostname']
+knox_host_port = config['configurations']['gateway-site']['gateway.port']
+topology_template = config['configurations']['topology']['content']
+admin_topology_template = default('/configurations/admin-topology/content', None)
+knoxsso_topology_template = config['configurations']['knoxsso-topology']['content']
+gateway_log4j = config['configurations']['gateway-log4j']['content']
+ldap_log4j = config['configurations']['ldap-log4j']['content']
+users_ldif = config['configurations']['users-ldif']['content']
+java_home = default("/configurations/cluster-env/tt_jdk_home", None)
+security_enabled = config['configurations']['cluster-env']['security_enabled']
+smokeuser = config['configurations']['cluster-env']['smokeuser']
+smokeuser_principal = config['configurations']['cluster-env']['smokeuser_principal_name']
+smoke_user_keytab = config['configurations']['cluster-env']['smokeuser_keytab']
+kinit_path_local = get_kinit_path(default('/configurations/kerberos-env/executable_search_paths', None))
+if security_enabled:
+    knox_keytab_path = config['configurations']['knox-env']['knox_keytab_path']
+    _hostname_lowercase = config['agentLevelParams']['hostname'].lower()
+    knox_principal_name = config['configurations']['knox-env']['knox_principal_name'].replace('_HOST',
+                                                                                              _hostname_lowercase)
+
+# for curl command in ranger plugin to get db connector
+jdk_location = config['ambariLevelParams']['jdk_location']
+
+# ranger knox plugin start section
+
+# ranger host
+ranger_admin_hosts = default("/clusterHostInfo/ranger_admin_hosts", [])
+has_ranger_admin = not len(ranger_admin_hosts) == 0
+
+# ranger support xml_configuration flag, instead of depending on ranger xml_configurations_supported/ranger-env, using stack feature
+xml_configurations_supported = check_stack_feature(StackFeature.RANGER_XML_CONFIGURATION,
+                                                   version_for_stack_feature_checks)
+
+# ranger knox plugin enabled property
+enable_ranger_knox = default("/configurations/ranger-knox-plugin-properties/ranger-knox-plugin-enabled", "No")
+enable_ranger_knox = True if enable_ranger_knox.lower() == 'yes' else False
+
+# get ranger knox properties if enable_ranger_knox is True
+if enable_ranger_knox:
+    # get ranger policy url
+    policymgr_mgr_url = config['configurations']['admin-properties']['policymgr_external_url']
+    if xml_configurations_supported:
+        policymgr_mgr_url = config['configurations']['ranger-knox-security']['ranger.plugin.knox.policy.rest.url']
+
+    if not is_empty(policymgr_mgr_url) and policymgr_mgr_url.endswith('/'):
+        policymgr_mgr_url = policymgr_mgr_url.rstrip('/')
+
+    # ranger audit db user
+    xa_audit_db_user = default('/configurations/admin-properties/audit_db_user', 'rangerlogger')
+
+    # ranger knox service/repositry name
+    repo_name = str(config['clusterName']) + '_knox'
+    repo_name_value = config['configurations']['ranger-knox-security']['ranger.plugin.knox.service.name']
+    if not is_empty(repo_name_value) and repo_name_value != "{{repo_name}}":
+        repo_name = repo_name_value
+
+    knox_home = config['configurations']['ranger-knox-plugin-properties']['KNOX_HOME']
+    common_name_for_certificate = config['configurations']['ranger-knox-plugin-properties'][
+        'common.name.for.certificate']
+    repo_config_username = config['configurations']['ranger-knox-plugin-properties']['REPOSITORY_CONFIG_USERNAME']
+
+    # ranger-env config
+    ranger_env = config['configurations']['ranger-env']
+
+    # create ranger-env config having external ranger credential properties
+    if not has_ranger_admin and enable_ranger_knox:
+        external_admin_username = default('/configurations/ranger-knox-plugin-properties/external_admin_username',
+                                          'admin')
+        external_admin_password = default('/configurations/ranger-knox-plugin-properties/external_admin_password',
+                                          'admin')
+        external_ranger_admin_username = default(
+            '/configurations/ranger-knox-plugin-properties/external_ranger_admin_username', 'amb_ranger_admin')
+        external_ranger_admin_password = default(
+            '/configurations/ranger-knox-plugin-properties/external_ranger_admin_password', 'amb_ranger_admin')
+        ranger_env = {}
+        ranger_env['admin_username'] = external_admin_username
+        ranger_env['admin_password'] = external_admin_password
+        ranger_env['ranger_admin_username'] = external_ranger_admin_username
+        ranger_env['ranger_admin_password'] = external_ranger_admin_password
+
+    ranger_plugin_properties = config['configurations']['ranger-knox-plugin-properties']
+    policy_user = config['configurations']['ranger-knox-plugin-properties']['policy_user']
+    repo_config_password = config['configurations']['ranger-knox-plugin-properties']['REPOSITORY_CONFIG_PASSWORD']
+
+    xa_audit_db_password = ''
+    if not is_empty(config['configurations']['admin-properties'][
+                        'audit_db_password']) and stack_supports_ranger_audit_db and has_ranger_admin:
+        xa_audit_db_password = config['configurations']['admin-properties']['audit_db_password']
+
+    downloaded_custom_connector = None
+    previous_jdbc_jar_name = None
+    driver_curl_source = None
+    driver_curl_target = None
+    previous_jdbc_jar = None
+
+    if has_ranger_admin and stack_supports_ranger_audit_db:
+        xa_audit_db_flavor = config['configurations']['admin-properties']['DB_FLAVOR']
+        jdbc_jar_name, previous_jdbc_jar_name, audit_jdbc_url, jdbc_driver = get_audit_configs(config)
+
+        downloaded_custom_connector = format("{tmp_dir}/{jdbc_jar_name}") if stack_supports_ranger_audit_db else None
+        driver_curl_source = format("{jdk_location}/{jdbc_jar_name}") if stack_supports_ranger_audit_db else None
+        driver_curl_target = format(
+            "{stack_root}/current/knox-server/ext/{jdbc_jar_name}") if stack_supports_ranger_audit_db else None
+        previous_jdbc_jar = format(
+            "{stack_root}/current/knox-server/ext/{previous_jdbc_jar_name}") if stack_supports_ranger_audit_db else None
+        sql_connector_jar = ''
+
+    knox_ranger_plugin_config = {
+        'username': repo_config_username,
+        'password': repo_config_password,
+        'knox.url': format("https://{knox_host_name}:{knox_host_port}/gateway/admin/api/v1/topologies"),
+        'commonNameForCertificate': common_name_for_certificate
+    }
+
+    if security_enabled:
+        knox_ranger_plugin_config['policy.download.auth.users'] = knox_user
+        knox_ranger_plugin_config['tag.download.auth.users'] = knox_user
+
+    custom_ranger_service_config = generate_ranger_service_config(ranger_plugin_properties)
+    if len(custom_ranger_service_config) > 0:
+        knox_ranger_plugin_config.update(custom_ranger_service_config)
+
+    knox_ranger_plugin_repo = {
+        'isEnabled': 'true',
+        'configs': knox_ranger_plugin_config,
+        'description': 'knox repo',
+        'name': repo_name,
+        'type': 'knox'
+    }
+
+    xa_audit_db_is_enabled = False
+    if xml_configurations_supported and stack_supports_ranger_audit_db:
+        xa_audit_db_is_enabled = config['configurations']['ranger-knox-audit']['xasecure.audit.destination.db']
+
+    xa_audit_hdfs_is_enabled = config['configurations']['ranger-knox-audit'][
+        'xasecure.audit.destination.hdfs'] if xml_configurations_supported else False
+    ssl_keystore_password = config['configurations']['ranger-knox-policymgr-ssl'][
+        'xasecure.policymgr.clientssl.keystore.password'] if xml_configurations_supported else None
+    ssl_truststore_password = config['configurations']['ranger-knox-policymgr-ssl'][
+        'xasecure.policymgr.clientssl.truststore.password'] if xml_configurations_supported else None
+    credential_file = format('/etc/ranger/{repo_name}/cred.jceks')
+
+    # for SQLA explicitly disable audit to DB for Ranger
+    if has_ranger_admin and stack_supports_ranger_audit_db and xa_audit_db_flavor == 'sqla':
+        xa_audit_db_is_enabled = False
+
+# need this to capture cluster name from where ranger knox plugin is enabled
+cluster_name = config['clusterName']
+
+# ranger knox plugin end section
+
+hdfs_user = config['configurations']['hadoop-env']['hdfs_user'] if has_namenode else None
+hdfs_user_keytab = config['configurations']['hadoop-env']['hdfs_user_keytab'] if has_namenode else None
+hdfs_principal_name = config['configurations']['hadoop-env']['hdfs_principal_name'] if has_namenode else None
+hdfs_site = config['configurations']['hdfs-site'] if has_namenode else None
+default_fs = config['configurations']['core-site']['fs.defaultFS'] if has_namenode else None
+hadoop_bin_dir = stack_select.get_hadoop_dir("bin") if has_namenode else None
+hadoop_conf_dir = conf_select.get_hadoop_conf_dir() if has_namenode else None
+
+import functools
+
+# create partial functions with common arguments for every HdfsResource call
+# to create/delete hdfs directory/file/copyfromlocal we need to call params.HdfsResource in code
+HdfsResource = functools.partial(
+    HdfsResource,
+    user=hdfs_user,
+    hdfs_resource_ignore_file="/var/lib/ambari-agent/data/.hdfs_resource_ignore",
+    security_enabled=security_enabled,
+    keytab=hdfs_user_keytab,
+    kinit_path_local=kinit_path_local,
+    hadoop_bin_dir=hadoop_bin_dir,
+    hadoop_conf_dir=hadoop_conf_dir,
+    principal_name=hdfs_principal_name,
+    hdfs_site=hdfs_site,
+    default_fs=default_fs,
+    immutable_paths=get_not_managed_resources(),
+    dfs_type=dfs_type
+)
+
+druid_coordinator_urls = ""
+if "druid-coordinator" in config['configurations']:
+    port = config['configurations']['druid-coordinator']['druid.port']
+    for host in config['clusterHostInfo']['druid_coordinator_hosts']:
+        druid_coordinator_urls += buildUrlElement("http", host, port, "")
+
+druid_overlord_urls = ""
+if "druid-overlord" in config['configurations']:
+    port = config['configurations']['druid-overlord']['druid.port']
+    for host in config['clusterHostInfo']['druid_overlord_hosts']:
+        druid_overlord_urls += buildUrlElement("http", host, port, "")
+
+druid_broker_urls = ""
+if "druid-broker" in config['configurations']:
+    port = config['configurations']['druid-broker']['druid.port']
+    for host in config['clusterHostInfo']['druid_broker_hosts']:
+        druid_broker_urls += buildUrlElement("http", host, port, "")
+
+druid_router_urls = ""
+if "druid-router" in config['configurations']:
+    port = config['configurations']['druid-router']['druid.port']
+    for host in config['clusterHostInfo']['druid_router_hosts']:
+        druid_router_urls += buildUrlElement("http", host, port, "")
+
+zeppelin_ui_urls = ""
+zeppelin_ws_urls = ""
+websocket_support = "false"
+if "zeppelin-site" in config['configurations']:
+    port = config['configurations']['zeppelin-site']['zeppelin.server.port']
+    protocol = "https" if config['configurations']['zeppelin-site']['zeppelin.ssl'] else "http"
+    host = config['clusterHostInfo']['zeppelin_master_hosts'][0]
+    zeppelin_ui_urls += buildUrlElement(protocol, host, port, "")
+    zeppelin_ws_urls += buildUrlElement("ws", host, port, "/ws")
+    websocket_support = "true"
+
+if "topology" in config['configurations']:
+    if 'ws://' in config['configurations']['topology']['content'] or 'wss://' in config['configurations']['topology'][
+        'content']:
+        websocket_support = "true"
+
+# for stack 3.0 +
+knox_descriptors_dir = format('{knox_conf_dir}/descriptors')
+knox_shared_providers_dir = format('{knox_conf_dir}/shared-providers')
+
+mount_table_xml_inclusion_file_full_path = None
+mount_table_content = None
+if 'viewfs-mount-table' in config['configurations']:
+    xml_inclusion_file_name = 'viewfs-mount-table.xml'
+    mount_table = config['configurations']['viewfs-mount-table']
+
+    if 'content' in mount_table and mount_table['content'].strip():
+        mount_table_xml_inclusion_file_full_path = os.path.join(knox_conf_dir, xml_inclusion_file_name)
+        mount_table_content = mount_table['content']
+
+# TTBigdata optimized
+
+
+#
+# Spark History UI (for Knox topology <url>... multiple)
+#
+spark_scheme = 'http'
+spark_historyserver_hosts = default("/clusterHostInfo/spark_jobhistoryserver_hosts", None)
+spark_historyserver_ui_port = default("/configurations/spark-defaults/spark.history.ui.port", "18081")
+
+spark_historyserver_urls = ""
+
+if spark_historyserver_hosts is None:
+    spark_history_hosts = []
+elif type(spark_historyserver_hosts) is list:
+    spark_history_hosts = spark_historyserver_hosts
+else:
+    spark_history_hosts = [spark_historyserver_hosts]
+
+for host in spark_history_hosts:
+    # History UI 一般不需要额外 path，若你想强制带 / 也可以把最后一个参数改成 "/"
+    spark_historyserver_urls += buildUrlElement(spark_scheme, host, spark_historyserver_ui_port, "")
+
+#
+# Solr
+#
+
+solr_scheme = 'http'
+solr_server_hosts = default("/clusterHostInfo/solr_server_hosts", None)
+solr_port = default("/configuration/solr/solr-env/solr_port", "8983")  # 如果你们实际是这个路径就保留
+solr_server_urls = ""
+if solr_server_hosts is None:
+    hosts = []
+elif type(solr_server_hosts) is list:
+    hosts = solr_server_hosts
+else:
+    hosts = [solr_server_hosts]
+for host in hosts:
+    solr_server_urls += buildUrlElement(solr_scheme, host, solr_port, "/solr/")
+
+
+#
+# Trino (Coordinator UI / API) for Knox topology
+#
+
+trino_coordinator_hosts = default("/clusterHostInfo/trino_coordinator_hosts", None)
+trino_https_enabled = default("/configurations/config.properties/http-server.https.enabled", "false")
+# 有些栈会把布尔值写成 True/False 或 true/false，这里统一处理
+trino_https_enabled_str = str(trino_https_enabled).strip().lower()
+trino_scheme = "https" if trino_https_enabled_str == "true" else "http"
+# 3) 读取端口（按 scheme 选 http 或 https 端口）
+trino_http_port = default("/configurations/config.properties/http-server.http.port", "8380")
+trino_https_port = default("/configurations/config.properties/http-server.https.port", "8443")
+trino_port = trino_https_port if trino_scheme == "https" else trino_http_port
+
+# 4) 组装 host 列表
+if trino_coordinator_hosts is None:
+    trino_hosts = []
+elif type(trino_coordinator_hosts) is list:
+    trino_hosts = trino_coordinator_hosts
+else:
+    trino_hosts = [trino_coordinator_hosts]
+
+# 5) 输出 URL Elements
+# - Web UI 一般是 /ui/
+trino_urls = ""
+for host in trino_hosts:
+    trino_urls += buildUrlElement(trino_scheme, host, trino_port, "")
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/metainfo.xml	(date 1765958704392)
@@ -21,7 +21,7 @@
     <service>
       <name>FLINK</name>
       <displayName>Flink</displayName>
-      <comment>Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams</comment>
+      <comment>【增强】分布式流处理与批处理计算引擎，用于实时计算、数据处理与作业调度，使用前需安装并部署 HDFS、ZooKeeper 等依赖组件</comment>
       <version>1.17.2</version>
       <components>
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params_windows.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params_windows.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params_windows.py
new file mode 100644
--- /dev/null	(date 1765958702660)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params_windows.py	(date 1765958702660)
@@ -0,0 +1,71 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+from resource_management.libraries.script.script import Script
+import os
+from status_params import *
+
+# server configurations
+config = Script.get_config()
+
+stack_root = None
+knox_home = None
+knox_conf_dir = None
+knox_logs_dir = None
+knox_bin = None
+ldap_bin = None
+knox_client_bin = None
+knox_data_dir = None
+
+knox_master_secret_path = None
+knox_cert_store_path = None
+
+try:
+  stack_root = os.path.abspath(os.path.join(os.environ["HADOOP_HOME"],".."))
+  knox_home = os.environ['KNOX_HOME']
+  knox_conf_dir = os.environ['KNOX_CONF_DIR']
+  knox_logs_dir = os.environ['KNOX_LOG_DIR']
+  knox_bin = os.path.join(knox_home, 'bin', 'gateway.exe')
+  ldap_bin = os.path.join(knox_home, 'bin', 'ldap.exe')
+  knox_client_bin = os.path.join(knox_home, 'bin', 'knoxcli.cmd')
+  knox_data_dir = os.path.join(knox_home, 'data')
+
+  knox_master_secret_path = os.path.join(knox_data_dir, 'security', 'master')
+  knox_cert_store_path = os.path.join(knox_data_dir, 'security', 'keystores', 'gateway.jks')
+except:
+  pass
+
+knox_host_port = config['configurations']['gateway-site']['gateway.port']
+knox_host_name = config['clusterHostInfo']['knox_gateway_hosts'][0]
+knox_host_name_in_cluster = config['agentLevelParams']['hostname']
+knox_master_secret = config['configurations']['knox-env']['knox_master_secret']
+topology_template = config['configurations']['topology']['content']
+admin_topology_template = default('/configurations/admin-topology/content', None)
+knoxsso_topology_template = config['configurations']['knoxsso-topology']['content']
+gateway_log4j = config['configurations']['gateway-log4j']['content']
+security_enabled = config['configurations']['cluster-env']['security_enabled']
+ldap_log4j = config['configurations']['ldap-log4j']['content']
+users_ldif = config['configurations']['users-ldif']['content']
+
+hadoop_user = config["configurations"]["cluster-env"]["hadoop.user.name"]
+knox_user = hadoop_user
+hdfs_user = hadoop_user
+knox_group = None
+mode = None
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/service_check.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/service_check.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/service_check.py
new file mode 100644
--- /dev/null	(date 1765958702638)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/service_check.py	(date 1765958702638)
@@ -0,0 +1,96 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+from resource_management.libraries.script.script import Script
+from resource_management.core.resources.system import Execute, File
+from resource_management.libraries.functions.format import format
+from resource_management.core.source import StaticFile
+import sys
+import os
+from ambari_commons import OSConst
+from ambari_commons.os_family_impl import OsFamilyFuncImpl, OsFamilyImpl
+
+
+class KnoxServiceCheck(Script):
+  def service_check(self, env):
+    pass
+
+
+@OsFamilyImpl(os_family=OSConst.WINSRV_FAMILY)
+class KnoxServiceCheckWindows(KnoxServiceCheck):
+  def service_check(self, env):
+    import params
+    env.set_params(params)
+
+    temp_dir = os.path.join(os.path.dirname(params.knox_home), "temp")
+    validateKnoxFileName = "validateKnoxStatus.py"
+    validateKnoxFilePath = os.path.join(temp_dir, validateKnoxFileName)
+    python_executable = sys.executable
+    validateStatusCmd = "%s %s -p %s -n %s" % (python_executable, validateKnoxFilePath, params.knox_host_port, params.knox_host_name)
+
+    print("Test connectivity to knox server")
+
+    File(validateKnoxFilePath,
+         content=StaticFile(validateKnoxFileName)
+    )
+
+    Execute(validateStatusCmd,
+            tries=3,
+            try_sleep=5,
+            timeout=5,
+            logoutput=True
+    )
+
+
+@OsFamilyImpl(os_family=OsFamilyImpl.DEFAULT)
+class KnoxServiceCheckDefault(KnoxServiceCheck):
+  def service_check(self, env):
+    import params
+    env.set_params(params)
+
+    validateKnoxFileName = "validateKnoxStatus.py"
+    validateKnoxFilePath = format("{tmp_dir}/{validateKnoxFileName}")
+    python_executable = sys.executable
+    validateStatusCmd = format("{python_executable} {validateKnoxFilePath} -p {knox_host_port} -n {knox_host_name}")
+    if params.security_enabled:
+      kinit_cmd = format("{kinit_path_local} -kt {smoke_user_keytab} {smokeuser_principal};")
+      smoke_cmd = format("{kinit_cmd} {validateStatusCmd}")
+    else:
+      smoke_cmd = validateStatusCmd
+
+    print("Test connectivity to knox server")
+
+    File(validateKnoxFilePath,
+         content=StaticFile(validateKnoxFileName),
+         mode=0o755
+    )
+
+    Execute(smoke_cmd,
+            tries=15,
+            try_sleep=5,
+            path='/usr/sbin:/sbin:/usr/local/bin:/bin:/usr/bin',
+            user=params.smokeuser,
+            timeout=5,
+            logoutput=True
+    )
+
+
+if __name__ == "__main__":
+    KnoxServiceCheck().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox_ldap.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox_ldap.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox_ldap.py
new file mode 100644
--- /dev/null	(date 1765958702297)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox_ldap.py	(date 1765958702297)
@@ -0,0 +1,60 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+import os
+from resource_management.libraries.script.script import Script
+from resource_management.core.resources.service import ServiceConfig
+from resource_management.core.resources.system import File
+from ambari_commons import OSConst
+from resource_management.core.source import InlineTemplate
+from ambari_commons.os_family_impl import OsFamilyFuncImpl, OsFamilyImpl
+
+def _ldap_common():
+    import params
+
+    File(os.path.join(params.knox_conf_dir, 'ldap-log4j.properties'),
+         mode=params.mode,
+         group=params.knox_group,
+         owner=params.knox_user,
+         content=InlineTemplate(params.ldap_log4j)
+    )
+
+    File(os.path.join(params.knox_conf_dir, 'users.ldif'),
+         mode=params.mode,
+         group=params.knox_group,
+         owner=params.knox_user,
+         content=params.users_ldif
+    )
+
+@OsFamilyFuncImpl(os_family=OSConst.WINSRV_FAMILY)
+def ldap():
+  import params
+
+  # Manually overriding service logon user & password set by the installation package
+  ServiceConfig(params.knox_ldap_win_service_name,
+                action="change_user",
+                username = params.knox_user,
+                password = Script.get_password(params.knox_user))
+
+  _ldap_common()
+
+@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
+def ldap():
+  _ldap_common()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/status_params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/status_params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/status_params.py
new file mode 100644
--- /dev/null	(date 1765958702656)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/status_params.py	(date 1765958702656)
@@ -0,0 +1,59 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+from resource_management.libraries.functions import format
+from resource_management.libraries.functions.default import default
+from resource_management.libraries.functions import get_kinit_path
+from resource_management.libraries.script.script import Script
+from ambari_commons import OSCheck
+from resource_management.libraries.functions.version import format_stack_version
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions import StackFeature
+
+
+config = Script.get_config()
+stack_root = Script.get_stack_root()
+stack_version_unformatted = config['clusterLevelParams']['stack_version']
+stack_version_formatted = format_stack_version(stack_version_unformatted)
+
+if OSCheck.is_windows_family():
+  knox_gateway_win_service_name = "gateway"
+  knox_ldap_win_service_name = "ldap"
+else:
+  knox_conf_dir = '/etc/knox/conf'
+  if stack_version_formatted and check_stack_feature(StackFeature.ROLLING_UPGRADE, stack_version_formatted):
+    knox_conf_dir = format('{stack_root}/current/knox-server/conf')
+  knox_pid_dir = config['configurations']['knox-env']['knox_pid_dir']
+  knox_pid_file = format("{knox_pid_dir}/gateway.pid")
+  ldap_pid_file = format("{knox_pid_dir}/ldap.pid")
+
+  security_enabled = config['configurations']['cluster-env']['security_enabled']
+  if security_enabled:
+      knox_keytab_path = config['configurations']['knox-env']['knox_keytab_path']
+      knox_principal_name = config['configurations']['knox-env']['knox_principal_name']
+  else:
+      knox_keytab_path = None
+      knox_principal_name = None
+
+  hostname = config['agentLevelParams']['hostname'].lower()
+  knox_user = default("/configurations/knox-env/knox_user", "knox")
+  kinit_path_local = get_kinit_path(default('/configurations/kerberos-env/executable_search_paths', None))
+  temp_dir = Script.get_tmp_dir()
+  
+stack_name = default("/clusterLevelParams/stack_name", None)
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/setup_ranger_knox.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/setup_ranger_knox.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/setup_ranger_knox.py
new file mode 100644
--- /dev/null	(date 1765958702664)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/setup_ranger_knox.py	(date 1765958702664)
@@ -0,0 +1,149 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+from resource_management.core.logger import Logger
+from resource_management.libraries.functions.setup_ranger_plugin_xml import setup_ranger_plugin
+from resource_management.libraries.functions.setup_ranger_plugin_xml import setup_configuration_file_for_required_plugins
+from resource_management.core.resources import File
+from resource_management.libraries.resources.xml_config import XmlConfig
+from resource_management.libraries.functions.format import format
+
+def setup_ranger_knox(upgrade_type=None):
+  import params
+
+  if params.enable_ranger_knox:
+
+    stack_version = None
+    if upgrade_type is not None:
+      stack_version = params.version
+
+    if params.retryAble:
+      Logger.info("Knox: Setup ranger: command retry enables thus retrying if ranger admin is down !")
+    else:
+      Logger.info("Knox: Setup ranger: command retry not enabled thus skipping if ranger admin is down !")
+
+    if params.xa_audit_hdfs_is_enabled:
+      if params.has_namenode:
+        try:
+          params.HdfsResource("/ranger/audit",
+                             type="directory",
+                             action="create_on_execute",
+                             owner=params.hdfs_user,
+                             group=params.hdfs_user,
+                             mode=0o755,
+                             recursive_chmod=True
+          )
+          params.HdfsResource("/ranger/audit/knox",
+                             type="directory",
+                             action="create_on_execute",
+                             owner=params.knox_user,
+                             group=params.knox_user,
+                             mode=0o700,
+                             recursive_chmod=True
+          )
+          params.HdfsResource(None, action="execute")
+        except Exception as err:
+          Logger.exception("Audit directory creation in HDFS for KNOX Ranger plugin failed with error:\n{0}".format(err))
+
+        if params.namenode_hosts is not None and len(params.namenode_hosts) > 1:
+          Logger.info('Ranger Knox plugin is enabled in NameNode HA environment along with audit to Hdfs enabled, creating hdfs-site.xml')
+          XmlConfig("hdfs-site.xml",
+            conf_dir=params.knox_conf_dir,
+            configurations=params.config['configurations']['hdfs-site'],
+            configuration_attributes=params.config['configurationAttributes']['hdfs-site'],
+            owner=params.knox_user,
+            group=params.knox_group,
+            mode=0o644
+          )
+        else:
+          File(format('{knox_conf_dir}/hdfs-site.xml'), action="delete")
+
+    api_version = 'v2'
+
+    setup_ranger_plugin('knox-server', 'knox', params.previous_jdbc_jar,
+                        params.downloaded_custom_connector, params.driver_curl_source,
+                        params.driver_curl_target, params.java_home,
+                        params.repo_name, params.knox_ranger_plugin_repo,
+                        params.ranger_env, params.ranger_plugin_properties,
+                        params.policy_user, params.policymgr_mgr_url,
+                        params.enable_ranger_knox, conf_dict=params.knox_conf_dir,
+                        component_user=params.knox_user, component_group=params.knox_group, cache_service_list=['knox'],
+                        plugin_audit_properties=params.config['configurations']['ranger-knox-audit'], plugin_audit_attributes=params.config['configurationAttributes']['ranger-knox-audit'],
+                        plugin_security_properties=params.config['configurations']['ranger-knox-security'], plugin_security_attributes=params.config['configurationAttributes']['ranger-knox-security'],
+                        plugin_policymgr_ssl_properties=params.config['configurations']['ranger-knox-policymgr-ssl'], plugin_policymgr_ssl_attributes=params.config['configurationAttributes']['ranger-knox-policymgr-ssl'],
+                        component_list=['knox-server'], audit_db_is_enabled=params.xa_audit_db_is_enabled,
+                        credential_file=params.credential_file, xa_audit_db_password=params.xa_audit_db_password,
+                        ssl_truststore_password=params.ssl_truststore_password, ssl_keystore_password=params.ssl_keystore_password,
+                        stack_version_override = stack_version, skip_if_rangeradmin_down= not params.retryAble,api_version=api_version,
+                        is_security_enabled = params.security_enabled,
+                        is_stack_supports_ranger_kerberos = params.stack_supports_ranger_kerberos,
+                        component_user_principal=params.knox_principal_name if params.security_enabled else None,
+                        component_user_keytab=params.knox_keytab_path if params.security_enabled else None)
+
+    if params.stack_supports_core_site_for_ranger_plugin and params.enable_ranger_knox and params.security_enabled:
+      if params.has_namenode:
+        Logger.info("Stack supports core-site.xml creation for Ranger plugin and Namenode is installed, creating create core-site.xml from namenode configurations")
+        setup_configuration_file_for_required_plugins(component_user = params.knox_user, component_group = params.knox_group,
+                                             create_core_site_path = params.knox_conf_dir, configurations = params.config['configurations']['core-site'],
+                                             configuration_attributes = params.config['configurationAttributes']['core-site'], file_name='core-site.xml',
+                                             xml_include_file=params.mount_table_xml_inclusion_file_full_path, xml_include_file_content=params.mount_table_content)
+      else:
+        Logger.info("Stack supports core-site.xml creation for Ranger plugin and Namenode is not installed, creating create core-site.xml from default configurations")
+        setup_configuration_file_for_required_plugins(component_user = params.knox_user, component_group = params.knox_group,
+                                             create_core_site_path = params.knox_conf_dir, configurations = { 'hadoop.security.authentication' : 'kerberos' if params.security_enabled else 'simple' },
+                                             configuration_attributes = {}, file_name='core-site.xml')
+    else:
+      Logger.info("Stack does not support core-site.xml creation for Ranger plugin, skipping core-site.xml configurations")
+
+    #fix ranger files not foud 
+    ranger_knox_audit = params.config['configurations']['ranger-knox-audit']
+    XmlConfig("ranger-knox-" + params.repo_name + "-audit.xml",
+          conf_dir=params.knox_conf_dir,
+          configurations=ranger_knox_audit,
+          owner=params.knox_user,
+          group=params.knox_group,
+          mode=0o755)
+  
+    ranger_knox_policymgr_ssl = params.config['configurations']['ranger-knox-policymgr-ssl']
+    XmlConfig("ranger-knox-audit.xml",
+          conf_dir=params.knox_conf_dir,
+          configurations=ranger_knox_policymgr_ssl,
+          owner=params.knox_user,
+          group=params.knox_group,
+          mode=0o755)
+  
+    ranger_knox_policymgr_ssl = params.config['configurations']['ranger-knox-policymgr-ssl']
+    XmlConfig("ranger-knox-" + params.repo_name + "-audit.xml",
+          conf_dir=params.knox_conf_dir,
+          configurations=ranger_knox_policymgr_ssl,
+          owner=params.knox_user,
+          group=params.knox_group,
+          mode=0o755)
+ 
+    ranger_knox_security = params.config['configurations']['ranger-knox-security']
+    XmlConfig("ranger-knox-" + params.repo_name + "-security.xml",
+          conf_dir=params.knox_conf_dir,
+          configurations=ranger_knox_security,
+          owner=params.knox_user,
+          group=params.knox_group,
+          mode=0o755)
+  else:
+    Logger.info('Ranger Knox plugin is not enabled')
+
+
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params.py
new file mode 100644
--- /dev/null	(date 1765958702289)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/params.py	(date 1765958702289)
@@ -0,0 +1,30 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+Ambari Agent
+
+"""
+from ambari_commons import OSCheck
+from resource_management.libraries.functions.default import default
+
+if OSCheck.is_windows_family():
+  from params_windows import *
+else:
+  from params_linux import *
+
+retryAble = default("/commandParams/command_retry_enabled", False)
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/upgrade.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/upgrade.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/upgrade.py
new file mode 100644
--- /dev/null	(date 1765958702293)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/upgrade.py	(date 1765958702293)
@@ -0,0 +1,147 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+import os
+import tempfile
+
+from resource_management.core import sudo
+from resource_management.core.logger import Logger
+from resource_management.core.exceptions import Fail
+from resource_management.libraries.functions import tar_archive
+from resource_management.libraries.functions import format
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions import StackFeature
+from resource_management.libraries.script.script import Script
+
+
+BACKUP_TEMP_DIR = "knox-upgrade-backup"
+BACKUP_DATA_ARCHIVE = "knox-data-backup.tar"
+STACK_ROOT_DEFAULT = Script.get_stack_root()
+
+
+def backup_data():
+  """
+  Backs up the knox data as part of the upgrade process.
+  :return: Returns the path to the absolute backup directory.
+  """
+  Logger.info('Backing up Knox data directory before upgrade...')
+  directoryMappings = _get_directory_mappings_during_upgrade()
+
+  Logger.info("Directory mappings to backup: {0}".format(str(directoryMappings)))
+
+  absolute_backup_dir = os.path.join(tempfile.gettempdir(), BACKUP_TEMP_DIR)
+  if not os.path.isdir(absolute_backup_dir):
+    os.makedirs(absolute_backup_dir)
+
+  for directory in directoryMappings:
+    if not os.path.isdir(directory):
+      raise Fail("Unable to backup missing directory {0}".format(directory))
+
+    archive = os.path.join(absolute_backup_dir, directoryMappings[directory])
+    Logger.info('Compressing {0} to {1}'.format(directory, archive))
+
+    if os.path.exists(archive):
+      os.remove(archive)
+
+    # backup the directory, following symlinks instead of including them
+    tar_archive.archive_directory_dereference(archive, directory)
+
+  return absolute_backup_dir
+
+
+def copytree(src, dst, exclude_sub_dirs=set(), force_replace=False):
+  """
+  Copy content of directory from source path to the target path with possibility to exclude some directories
+
+  :type src str
+  :type dst str
+  :type exclude_sub_dirs list|set
+  :type force_replace bool
+  """
+
+  def copy(_src, _dst):
+    from resource_management.core import shell
+    # full path is required to avoid usage of system command aliases like 'cp -i', which block overwrite
+    if force_replace:
+      shell.checked_call(["/bin/cp", "-rfp", _src, _dst], sudo=True)
+    else:
+      shell.checked_call(["/bin/cp", "-rp", _src, _dst], sudo=True)
+
+  if not sudo.path_isdir(src) or not sudo.path_isdir(dst):
+    raise Fail("The source or the destination is not a folder")
+
+  sub_dirs_to_copy = sudo.listdir(src)
+  for d in sub_dirs_to_copy:
+    if d not in exclude_sub_dirs:
+      src_path = os.path.join(src, d)
+      copy(src_path, dst)
+
+
+def seed_current_data_directory():
+  """
+  HDP stack example:
+
+  Knox uses "versioned" data directories in some stacks:
+  /usr/hdp/2.2.0.0-1234/knox/data -> /var/lib/knox/data
+  /usr/hdp/2.3.0.0-4567/knox/data -> /var/lib/knox/data-2.3.0.0-4567
+
+  If the stack being upgraded to supports versioned data directories for Knox, then we should
+  seed the data from the prior version. This is mainly because Knox keeps things like keystores
+  in the data directory and if those aren't copied over then it will re-create self-signed
+  versions. This side-effect behavior causes loss of service in clusters where Knox is using
+  custom keystores.
+
+  cp -R -p -f /usr/hdp/<old>/knox-server/data/. /usr/hdp/current/knox-server/data
+  :return:
+  """
+  import params
+
+  if params.version is None or params.upgrade_from_version is None:
+    raise Fail("The source and target versions are required")
+
+  if check_stack_feature(StackFeature.KNOX_VERSIONED_DATA_DIR, params.version):
+    Logger.info("Seeding Knox data from prior version...")
+
+    application_dir_name = "applications"
+    exclude_root_dirs = [application_dir_name, "deployments"]
+    exclude_applications_from_copy = ["admin-ui", "knoxauth"]
+    source_data_dir = os.path.join(params.stack_root, params.upgrade_from_version, "knox", "data")
+    target_data_dir = os.path.join(params.stack_root, "current", "knox-server", "data")
+
+    copytree(source_data_dir, target_data_dir, exclude_root_dirs, True)
+    copytree(os.path.join(source_data_dir, application_dir_name), os.path.join(target_data_dir, application_dir_name),
+             exclude_applications_from_copy, True)
+
+
+def _get_directory_mappings_during_upgrade():
+  """
+  Gets a dictionary of directory to archive name that represents the
+  directories that need to be backed up and their output tarball archive targets
+  :return:  the dictionary of directory to tarball mappings
+  """
+  import params
+
+  # the data directory is always a symlink to the "correct" data directory in /var/lib/knox
+  # such as /var/lib/knox/data or /var/lib/knox/data-2.4.0.0-1234
+  knox_data_dir = STACK_ROOT_DEFAULT + '/current/knox-server/data'
+
+  directories = { knox_data_dir: BACKUP_DATA_ARCHIVE }
+
+  Logger.info(format("Knox directories to backup:\n{directories}"))
+  return directories
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox.py
new file mode 100644
--- /dev/null	(date 1765958702285)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/scripts/knox.py	(date 1765958702285)
@@ -0,0 +1,233 @@
+#!/usr/bin/env python3
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+import os
+from resource_management.libraries.script.script import Script
+from resource_management.libraries.resources.xml_config import XmlConfig
+from resource_management.core.resources.service import ServiceConfig
+from resource_management.libraries.functions.default import default
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.get_config import get_config
+from resource_management.libraries.functions.generate_logfeeder_input_config import generate_logfeeder_input_config
+from resource_management.libraries.resources.template_config import TemplateConfig
+from resource_management.core.resources.system import File, Execute, Directory
+from resource_management.core.shell import as_user
+from resource_management.core.source import Template, InlineTemplate
+
+from ambari_commons import OSConst
+from ambari_commons.os_family_impl import OsFamilyFuncImpl, OsFamilyImpl
+
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions import StackFeature
+
+@OsFamilyFuncImpl(os_family=OSConst.WINSRV_FAMILY)
+def knox():
+  import params
+
+  XmlConfig("gateway-site.xml",
+            conf_dir=params.knox_conf_dir,
+            configurations=params.config['configurations']['gateway-site'],
+            configuration_attributes=params.config['configurationAttributes']['gateway-site'],
+            owner=params.knox_user
+  )
+
+  # Manually overriding service logon user & password set by the installation package
+  ServiceConfig(params.knox_gateway_win_service_name,
+                action="change_user",
+                username = params.knox_user,
+                password = Script.get_password(params.knox_user))
+
+  File(os.path.join(params.knox_conf_dir, "gateway-log4j.properties"),
+       owner=params.knox_user,
+       content=params.gateway_log4j
+  )
+
+  File(os.path.join(params.knox_conf_dir, "topologies", "default.xml"),
+       mode=0o600,
+       group=params.knox_group,
+       owner=params.knox_user,
+       content=InlineTemplate(params.topology_template)
+  )
+
+  if params.admin_topology_template:
+    File(os.path.join(params.knox_conf_dir, "topologies", "admin.xml"),
+       mode=0o600,
+       group=params.knox_group,
+       owner=params.knox_user,
+       content=InlineTemplate(params.admin_topology_template)
+    )
+
+  if params.version_formatted and check_stack_feature(StackFeature.KNOX_SSO_TOPOLOGY, params.version_formatted):
+    knoxsso_topology_template_content = get_config("knoxsso-topology")
+    if knoxsso_topology_template_content:
+      File(os.path.join(params.knox_conf_dir, "topologies", "knoxsso.xml"),
+        mode=0o600,
+        group=params.knox_group,
+        owner=params.knox_user,
+        content=InlineTemplate(params.knoxsso_topology_template)
+      )
+
+  if params.security_enabled:
+    TemplateConfig( os.path.join(params.knox_conf_dir, "krb5JAASLogin.conf"),
+        owner = params.knox_user,
+        template_tag = None
+    )
+
+  if not os.path.isfile(params.knox_master_secret_path):
+    cmd = format('cmd /C {knox_client_bin} create-master --master {knox_master_secret!p}')
+    Execute(cmd)
+    cmd = format('cmd /C {knox_client_bin} create-cert --hostname {knox_host_name_in_cluster}')
+    Execute(cmd)
+
+@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
+def knox():
+    import params
+    Directory([params.knox_data_dir, params.knox_logs_dir, params.knox_pid_dir, params.knox_conf_dir, os.path.join(params.knox_conf_dir, "topologies"), params.knox_descriptors_dir, params.knox_shared_providers_dir],
+              owner = params.knox_user,
+              group = params.knox_group,
+              create_parents = True,
+              cd_access = "a",
+              mode = 0o755,
+              recursive_ownership = True,
+    )
+
+    XmlConfig("gateway-site.xml",
+              conf_dir=params.knox_conf_dir,
+              configurations=params.config['configurations']['gateway-site'],
+              configuration_attributes=params.config['configurationAttributes']['gateway-site'],
+              owner=params.knox_user,
+              group=params.knox_group,
+    )
+
+    # start knox 2.0
+    File(format("{params.knox_conf_dir}/gateway-log4j2.xml"),
+             owner=params.knox_user,
+             group=params.knox_group,
+             content=InlineTemplate(params.config['configurations']['gateway-log4j2']['content']),
+             mode=0o644,
+        )
+    File(format("{params.knox_conf_dir}/knoxcli-log4j2.xml"),
+             owner=params.knox_user,
+             group=params.knox_group,
+             content=InlineTemplate(params.config['configurations']['knoxcli-log4j2']['content']),
+             mode=0o644,
+        )
+    File(format("{params.knox_conf_dir}/ldap-log4j2.xml"),
+             owner=params.knox_user,
+             group=params.knox_group,
+             content=InlineTemplate(params.config['configurations']['ldap-log4j2']['content']),
+             mode=0o644,
+        )
+    File(format("{params.knox_conf_dir}/shell-log4j2.xml"),
+             owner=params.knox_user,
+             group=params.knox_group,
+             content=InlineTemplate(params.config['configurations']['shell-log4j2']['content']),
+             mode=0o644,
+        )
+    #end knox 2.0
+
+    File(format("{params.knox_conf_dir}/gateway-log4j.properties"),
+         mode=0o644,
+         group=params.knox_group,
+         owner=params.knox_user,
+         content=InlineTemplate(params.gateway_log4j)
+    )
+
+    File(format("{params.knox_conf_dir}/topologies/default.xml"),
+         mode=0o600,
+         group=params.knox_group,
+         owner=params.knox_user,
+         content=InlineTemplate(params.topology_template)
+    )
+
+    if params.admin_topology_template:
+      File(format("{params.knox_conf_dir}/topologies/admin.xml"),
+           mode=0o600,
+           group=params.knox_group,
+           owner=params.knox_user,
+           content=InlineTemplate(params.admin_topology_template)
+      )
+
+    if params.version_formatted and check_stack_feature(StackFeature.KNOX_SSO_TOPOLOGY, params.version_formatted):
+      knoxsso_topology_template_content = get_config("knoxsso-topology")
+      if knoxsso_topology_template_content:
+        File(os.path.join(params.knox_conf_dir, "topologies", "knoxsso.xml"),
+            mode=0o600,
+            group=params.knox_group,
+            owner=params.knox_user,
+            content=InlineTemplate(params.knoxsso_topology_template)
+        )
+
+    if params.security_enabled:
+      TemplateConfig( format("{knox_conf_dir}/krb5JAASLogin.conf"),
+                      owner = params.knox_user,
+                      template_tag = None
+      )
+
+    generate_logfeeder_input_config('knox', Template("input.config-knox.json.j2", extra_imports=[default]))
+
+    cmd = format('{knox_client_bin} create-master --master {knox_master_secret!p}')
+    master_secret_exist = as_user(format('test -f {knox_master_secret_path}'), params.knox_user)
+
+    Execute(cmd,
+            user=params.knox_user,
+            environment={'JAVA_HOME': params.java_home},
+            not_if=master_secret_exist,
+    )
+
+    cmd = format('{knox_client_bin} create-cert --hostname {knox_host_name_in_cluster}')
+    cert_store_exist = as_user(format('test -f {knox_cert_store_path}'), params.knox_user)
+
+    Execute(cmd,
+            user=params.knox_user,
+            environment={'JAVA_HOME': params.java_home},
+            not_if=cert_store_exist,
+    )
+
+
+
+@OsFamilyFuncImpl(os_family=OSConst.WINSRV_FAMILY)
+def update_knox_folder_permissions():
+  import params
+  Directory(params.knox_logs_dir,
+            owner = params.knox_user,
+            group = params.knox_group
+            )
+
+
+@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
+def update_knox_logfolder_permissions():
+  """
+   Fix for the bug with rpm/deb packages. During installation of the package, they re-apply permissions to the
+   folders below; such behaviour will affect installations with non-standard user name/group and will put
+   cluster in non-working state
+  """
+  import params
+  
+  Directory(params.knox_logs_dir,
+            owner = params.knox_user,
+            group = params.knox_group,
+            create_parents = True,
+            cd_access = "a",
+            mode=0o700,
+            recursive_ownership = True,
+  )
+
+
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/templates/input.config-knox.json.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/templates/input.config-knox.json.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/templates/input.config-knox.json.j2
new file mode 100644
--- /dev/null	(date 1765958702671)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/templates/input.config-knox.json.j2	(date 1765958702671)
@@ -0,0 +1,60 @@
+{#
+ # Licensed to the Apache Software Foundation (ASF) under one
+ # or more contributor license agreements.  See the NOTICE file
+ # distributed with this work for additional information
+ # regarding copyright ownership.  The ASF licenses this file
+ # to you under the Apache License, Version 2.0 (the
+ # "License"); you may not use this file except in compliance
+ # with the License.  You may obtain a copy of the License at
+ #
+ #   http://www.apache.org/licenses/LICENSE-2.0
+ #
+ # Unless required by applicable law or agreed to in writing, software
+ # distributed under the License is distributed on an "AS IS" BASIS,
+ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ # See the License for the specific language governing permissions and
+ # limitations under the License.
+ #}
+{
+  "input":[
+    {
+      "type":"knox_gateway",
+      "rowtype":"service",
+      "path":"/var/log/knox/gateway.log"
+    },
+    {
+      "type":"knox_cli",
+      "rowtype":"service",
+      "path":"/var/log/knox/knoxcli.log"
+    },
+    {
+      "type":"knox_ldap",
+      "rowtype":"service",
+      "path":"/var/log/knox/ldap.log"
+    }
+  ],
+  "filter":[
+    {
+      "filter":"grok",
+      "conditions":{
+        "fields":{
+          "type":[
+            "knox_gateway",
+            "knox_cli",
+            "knox_ldap"
+          ]
+        }
+      },
+      "log4j_format":"%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n",
+      "multiline_pattern":"^(%{TIMESTAMP_ISO8601:logtime})",
+      "message_pattern":"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\(%{JAVAFILE:file}:%{JAVAMETHOD:method}\\(%{INT:line_number}\\)\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}",
+      "post_map_values":{
+        "logtime":{
+          "map_date":{
+            "target_date_pattern":"yyyy-MM-dd HH:mm:ss,SSS"
+          }
+        }
+      }
+    }
+  ]
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/templates/config.properties.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/templates/config.properties.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/templates/config.properties.j2
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/templates/config.properties.j2	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/package/templates/config.properties.j2	(date 1765958706084)
@@ -72,10 +72,10 @@
 node-scheduler.include-coordinator={{ config_properties['node-scheduler.include-coordinator'] }}
 
 # Maximum total memory that a query can use
-query.max-memory={{ config_properties['query.max-memory'] }}
+query.max-memory={{ config_properties['query.max-memory-gb'] }}GB
 
 # Maximum memory that a query can use on a single node
-query.max-memory-per-node={{ config_properties['query.max-memory-per-node'] }}
+query.max-memory-per-node={{ config_properties['query.max-memory-per-node-gb'] }}GB
 
 # Whether to enable spilling to disk
 spill-enabled={{ config_properties['spill-enabled'] }}
@@ -85,3 +85,8 @@
 
 # Directory where Trino plugins are located
 plugin.dir={{ config_properties['plugin.dir'] }}
+
+
+http-server.process-forwarded={{ config_properties['http-server.process-forwarded'] }}
+
+web-ui.authentication.type={{ config_properties['web-ui.authentication.type'] }}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/templates/krb5JAASLogin.conf.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/templates/krb5JAASLogin.conf.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/templates/krb5JAASLogin.conf.j2
new file mode 100644
--- /dev/null	(date 1765958702667)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/package/templates/krb5JAASLogin.conf.j2	(date 1765958702667)
@@ -0,0 +1,27 @@
+{#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#}
+com.sun.security.jgss.initiate {
+com.sun.security.auth.module.Krb5LoginModule required
+renewTGT=false
+doNotPrompt=true
+useKeyTab=true
+keyTab="{{knox_keytab_path}}"
+principal="{{knox_principal_name}}"
+storeKey=true
+useTicketCache=false;
+};
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/quicklinks/quicklinks.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/quicklinks/quicklinks.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/quicklinks/quicklinks.json
new file mode 100644
--- /dev/null	(date 1765958702676)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/quicklinks/quicklinks.json	(date 1765958702676)
@@ -0,0 +1,35 @@
+{
+  "name": "default",
+  "description": "default quick links configuration",
+  "configuration": {
+    "protocol": {
+      "type": "HTTPS_ONLY"
+    },
+    "links": [
+      {
+        "name": "knox_admin_ui",
+        "label": "Knox Admin UI",
+        "component_name": "KNOX_GATEWAY",
+        "url": "%@://%@:%@/${gateway-site/gateway.path}/manager/admin-ui",
+        "port": {
+          "https_property": "gateway.port",
+          "https_default_port": "8443",
+          "regex": "^(\\d+)$",
+          "site": "gateway-site"
+        }
+      },
+	  {
+        "name": "knox_home_ui",
+        "label": "Knox Home UI",
+        "component_name": "KNOX_GATEWAY",
+        "url": "%@://%@:%@/${gateway-site/gateway.path}/homepage/home",
+        "port": {
+          "https_property": "gateway.port",
+          "https_default_port": "8443",
+          "regex": "^(\\d+)$",
+          "site": "gateway-site"
+        }
+      }
+    ]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ldap-log4j.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ldap-log4j.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ldap-log4j.xml
new file mode 100644
--- /dev/null	(date 1765958702697)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ldap-log4j.xml	(date 1765958702697)
@@ -0,0 +1,93 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software<display-name> template</display-name>
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="false">
+   <property>
+    <name>knox_ldap_log_maxfilesize</name>
+    <value>256</value>
+    <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>Knox LDAP Log: backup file size</display-name>
+<value-attributes>
+      <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>knox_ldap_log_maxbackupindex</name>
+    <value>20</value>
+    <description>The number of backup files</description>
+    <display-name>Knox LDAP Log: # of backup files</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>ldap-log4j template</display-name>
+    <value>
+        # Licensed to the Apache Software Foundation (ASF) under one
+        # or more contributor license agreements.  See the NOTICE file
+        # distributed with this work for additional information
+        # regarding copyright ownership.  The ASF licenses this file
+        # to you under the Apache License, Version 2.0 (the
+        # "License"); you may not use this file except in compliance
+        # with the License.  You may obtain a copy of the License at
+        #
+        #     http://www.apache.org/licenses/LICENSE-2.0
+        #
+        # Unless required by applicable law or agreed to in writing, software
+        # distributed under the License is distributed on an "AS IS" BASIS,
+        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+        # See the License for the specific language governing permissions and
+        # limitations under the License.
+
+        app.log.dir=${launcher.dir}/../logs
+        app.log.file=${launcher.name}.log
+
+        log4j.rootLogger=ERROR, drfa
+        log4j.logger.org.apache.directory.server.ldap.LdapServer=INFO
+        log4j.logger.org.apache.directory=WARN
+
+        log4j.appender.stdout=org.apache.log4j.ConsoleAppender
+        log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
+        log4j.appender.stdout.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
+
+        log4j.appender.drfa=org.apache.log4j.DailyRollingFileAppender
+        log4j.appender.drfa.File=${app.log.dir}/${app.log.file}
+        log4j.appender.drfa.DatePattern=.yyyy-MM-dd
+        log4j.appender.drfa.layout=org.apache.log4j.PatternLayout
+        log4j.appender.drfa.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+        log4j.appender.drfa.MaxFileSize = {{knox_ldap_log_maxfilesize}}MB
+        log4j.appender.drfa.MaxBackupIndex = {{knox_ldap_log_maxbackupindex}}
+
+    </value>
+    <description>
+      content for log4j.properties file for the demo LDAP that comes with Knox.
+    </description>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/quicklinks/quicklinks.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/quicklinks/quicklinks.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/quicklinks/quicklinks.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/quicklinks/quicklinks.json	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TRINO/quicklinks/quicklinks.json	(date 1765958706094)
@@ -1,37 +1,36 @@
 {
-    "name": "default",
-    "description": "default quick links configuration",
-    "configuration": {
-      "protocol":
-      {
-        "type":"http",
-        "checks":[
-        ]
-      },
-      "links": [
-        {
-          "name": "trino_ui",
-          "label": "Trino UI",
-          "requires_user_name": "false",
-          "component_name": "TRINO_COORDINATOR",
-          "url":"%@://%@:%@",
-          "port":{
-            "http_property": "http-server.http.port",
-            "http_default_port": "8280",
-            "regex": "^(\\d+)$",
-            "site": "config-properties"
-          },
-          "protocol":{
-            "type":"https",
-            "checks":[
-              {
-                "property":"http-server.https.enabled",
-                "desired":"true",
-                "site":"config-properties"
-              }
-             ]
-         }
-        }
-      ]
-    }
-  }
\ No newline at end of file
+  "name": "default",
+  "description": "default quick links configuration",
+  "configuration": {
+    "protocol": {
+      "type": "http",
+      "checks": [
+      ]
+    },
+    "links": [
+      {
+        "name": "trino_ui",
+        "label": "Trino UI",
+        "requires_user_name": "false",
+        "component_name": "TRINO_COORDINATOR",
+        "url": "%@://%@:%@",
+        "port": {
+          "http_property": "http-server.http.port",
+          "http_default_port": "8280",
+          "regex": "^(\\d+)$",
+          "site": "config-properties"
+        },
+        "protocol": {
+          "type": "https",
+          "checks": [
+            {
+              "property": "http-server.https.enabled",
+              "desired": "true",
+              "site": "config-properties"
+            }
+          ]
+        }
+      }
+    ]
+  }
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knox-env.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knox-env.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knox-env.xml
new file mode 100644
--- /dev/null	(date 1765958702689)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knox-env.xml	(date 1765958702689)
@@ -0,0 +1,93 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <!-- knox-env.sh -->
+  <property require-input="true">
+    <name>knox_master_secret</name>
+    <value/>
+    <display-name>Knox Master Secret</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>password to use as the master secret</description>
+    <value-attributes>
+      <type>password</type>
+      <editable-only-at-install>true</editable-only-at-install>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>knox_user</name>
+    <display-name>Knox User</display-name>
+    <value>knox</value>
+    <property-type>USER</property-type>
+    <description>Knox Username.</description>
+    <value-attributes>
+      <type>user</type>
+      <overridable>false</overridable>
+      <user-groups>
+        <property>
+          <type>cluster-env</type>
+          <name>user_group</name>
+        </property>
+        <property>
+          <type>knox-env</type>
+          <name>knox_group</name>
+        </property>
+      </user-groups>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>knox_group</name>
+    <display-name>Knox Group</display-name>
+    <value>hadoop</value>
+    <property-type>GROUP</property-type>
+    <description>Knox Group.</description>
+    <value-attributes>
+      <type>user</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>knox_pid_dir</name>
+    <value>/var/run/knox</value>
+    <display-name>Knox PID dir</display-name>
+    <description>Knox PID dir.</description>
+    <value-attributes>
+      <type>directory</type>
+      <editable-only-at-install>true</editable-only-at-install>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>knox_principal_name</name>
+    <description>Knox principal name</description>
+    <property-type>KERBEROS_PRINCIPAL</property-type>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>knox_keytab_path</name>
+    <description>Knox keytab path</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-web/app/controllers/wizard/step4_controller.js
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-web/app/controllers/wizard/step4_controller.js b/ambari-web/app/controllers/wizard/step4_controller.js
--- a/ambari-web/app/controllers/wizard/step4_controller.js	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-web/app/controllers/wizard/step4_controller.js	(date 1765937489912)
@@ -28,6 +28,125 @@
    */
   content: [],
 
+  serviceOrderMap: {
+    'ZOOKEEPER': 10,
+    'HDFS': 20,
+    'YARN': 30,
+    'MAPREDUCE2': 40,
+    'TEZ': 50,
+    'HIVE': 60,
+    'SQOOP': 70,
+    'HBASE': 80,
+    'KAFKA': 85,
+    'DOLPHINSCHEDULER': 88,
+    'FLINK': 91,
+    'SPARK': 92,
+    'ZEPPELIN': 93,
+    'DORIS': 94,
+    'CELEBORN': 95,
+    'OZONE': 96,
+    'IMPALA': 97,
+    'SOLR': 98,
+    'CLOUDBEAVER': 99,
+    'RANGER': 100,
+    'TRINO': 101,
+    'PAIMON': 102,
+    'HUDI': 103,
+    'ATLAS': 104,
+    'SUPERSET': 105,
+    'LIVY': 106,
+    'ALLUXIO': 107,
+    'HUE': 108,
+    'KNOX': 109,
+    'AMBARI_METRICS': 110,
+    'RANGER_KMS': 201,
+    'VICTORIAMETRICS': 202,
+    'NIGHTINGALE': 203,
+    'AMBARI_INFRA_SOLR': 204,
+  },
+
+  _isSortingServices: false,
+  _sortScheduled: false,
+  _suppressResortObserver: false,
+
+  _getOrderWeight: function (svc) {
+    var map = this.get('serviceOrderMap') || {};
+    var name = svc && svc.get ? svc.get('serviceName') : (svc ? svc.serviceName : '');
+    return (map[name] !== undefined) ? map[name] : 9999;
+  },
+
+  _isAlreadySorted: function (arr) {
+    for (var i = 1; i < arr.length; i++) {
+      var prev = arr[i - 1];
+      var cur = arr[i];
+
+      var po = this._getOrderWeight(prev);
+      var co = this._getOrderWeight(cur);
+      if (po > co) return false;
+
+      if (po === co) {
+        var pd = prev.get('displayNameOnSelectServicePage') || '';
+        var cd = cur.get('displayNameOnSelectServicePage') || '';
+        if (String(pd).localeCompare(String(cd)) > 0) return false;
+      }
+    }
+    return true;
+  },
+
+  sortServicesInContent: function () {
+    if (this._isSortingServices) return;
+    this._isSortingServices = true;
+
+    try {
+      var content = this.get('content');
+      if (!content) return;
+
+      var arr = content.toArray ? content.toArray() : (content.slice ? content.slice() : []);
+      if (!arr || arr.length < 2) return;
+
+      // 关键：已经有序就什么都不做（避免 replace 触发下一轮 observer）
+      if (this._isAlreadySorted(arr)) return;
+
+      var self = this;
+      arr.sort(function (a, b) {
+        var ao = self._getOrderWeight(a);
+        var bo = self._getOrderWeight(b);
+        if (ao !== bo) return ao - bo;
+
+        var ad = a.get('displayNameOnSelectServicePage') || '';
+        var bd = b.get('displayNameOnSelectServicePage') || '';
+        return String(ad).localeCompare(String(bd));
+      });
+
+      // 关键：replace 时屏蔽 observer，防止回环
+      this._suppressResortObserver = true;
+      if (content.replace && content.get) {
+        content.replace(0, content.get('length'), arr);
+      } else {
+        this.set('content', Em.A(arr));
+      }
+      var that = this;
+      Em.run.next(function () {
+        that._suppressResortObserver = false;
+      });
+
+    } finally {
+      this._isSortingServices = false;
+    }
+  },
+
+  _resortOnContentChange: function () {
+    if (this._suppressResortObserver) return;
+    if (this._sortScheduled) return;
+
+    this._sortScheduled = true;
+    Em.run.next(this, function () {
+      this._sortScheduled = false;
+      this.sortServicesInContent();
+    });
+  }.observes('content.[]'),
+
+
   /**
    * Check / Uncheck 'Select All' checkbox with one argument; Check / Uncheck all other checkboxes with more arguments
    * @type {bool}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-site.xml
new file mode 100644
--- /dev/null	(date 1765958702708)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-site.xml	(date 1765958702708)
@@ -0,0 +1,160 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!-- 
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+<!-- The default settings for Knox. -->
+<!-- Edit gateway-site.xml to change settings for your local -->
+<!-- install. -->
+<configuration supports_final="false">
+    <property>
+        <name>gateway.port</name>
+        <value>6543</value>
+        <description>The HTTP port for the Gateway.</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>gateway.path</name>
+        <value>gateway</value>
+        <description>The default context path for the gateway.</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>gateway.gateway.conf.dir</name>
+        <display-name>Gateway Conf directory</display-name>
+        <value>deployments</value>
+        <description>The directory within GATEWAY_HOME that contains gateway topology files and deployments.
+        </description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>gateway.hadoop.kerberos.secured</name>
+        <value>false</value>
+        <description>Boolean flag indicating whether the Hadoop cluster protected by Gateway is secured with Kerberos
+        </description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>java.security.krb5.conf</name>
+        <value>/etc/knox/conf/krb5.conf</value>
+        <description>Absolute path to krb5.conf file</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>java.security.auth.login.config</name>
+        <value>/etc/knox/conf/krb5JAASLogin.conf</value>
+        <description>Absolute path to JASS login config file</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>sun.security.krb5.debug</name>
+        <value>false</value>
+        <description>Boolean flag indicating whether to enable debug messages for krb5 authentication</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>gateway.websocket.feature.enabled</name>
+        <value>{{websocket_support}}</value>
+        <description>Enable this if you want websocket support</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>gateway.knox.admin.users</name>
+        <value>admin</value>
+        <description>Comma separated list of usernames to grant access to Knox Admin UI and API</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>gateway.knox.admin.groups</name>
+        <value>admin</value>
+        <description>Comma separated list of groups to grant access to Knox Admin UI and API</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>gateway.dispatch.whitelist.services</name>
+        <value>DATANODE,HBASEUI,HDFSUI,JOBHISTORYUI,NODEUI,YARNUI,knoxauth,SOLR</value>
+        <description>The comma-delimited list of service roles for which the gateway.dispatch.whitelist should be
+            applied.
+        </description>
+    </property>
+    <property>
+        <name>gateway.dispatch.whitelist</name>
+        <value>DEFAULT</value>
+        <description>The whitelist to be applied for dispatches associated with the service roles specified by
+            gateway.dispatch.whitelist.services. If the value is DEFAULT, a domain-based whitelist will be derived from
+            the Knox host.
+        </description>
+    </property>
+    <property>
+        <name>gateway.read.only.override.topologies</name>
+        <value>admin,knoxsso,default</value>
+        <description>Since Ambari manages these topologies let's make them read-only in the Knox Admin UI so that they
+            don't get overwritten by Ambari on restart.
+        </description>
+    </property>
+
+    <property>
+        <name>gateway.httpclient.truststore.password.alias</name>
+        <value>trino-httpclient-truststore-pwd</value>
+        <description>
+            Knox 作为 HTTP Client 访问后端服务（例如 Trino/HTTPS）时，用于读取 Truststore 密码的“凭据别名”。
+            该别名对应 Knox Credential Store（例如 JCEKS）里保存的密码条目，而不是明文密码本身。
+            注意：value 里不要包含空格（尤其是尾部空格），否则可能导致找不到凭据条目。
+        </description>
+        <value-attributes>
+            <type>string</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+
+    <property>
+        <name>gateway.httpclient.truststore.type</name>
+        <value>JKS</value>
+        <description>
+            Truststore 文件类型。常见可选值：JKS、PKCS12。
+            如果你们使用的是 .p12/.pfx 文件，请改为 PKCS12 并同步修改 truststore.path 后缀。
+        </description>
+        <value-attributes>
+            <type>value-list</type>
+            <entries>
+                <entry>
+                    <value>JKS</value>
+                    <label>JKS</label>
+                </entry>
+                <entry>
+                    <value>PKCS12</value>
+                    <label>PKCS12</label>
+                </entry>
+            </entries>
+            <selection-cardinality>1</selection-cardinality>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+
+    <property>
+        <name>gateway.httpclient.truststore.path</name>
+        <value>/usr/bigtop/current/knox-server/data/security/keystores/trino-trust.jks</value>
+        <description>
+            Knox HTTP Client 使用的 Truststore 文件路径，用于信任后端服务的 HTTPS 证书链（例如 Trino 的 HTTPS 证书）。
+            建议保证该文件对 Knox 运行用户可读，并且在所有 Knox 节点上路径一致（如果是 HA/多节点部署）。
+        </description>
+        <value-attributes>
+            <type>string</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZOOKEEPER/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZOOKEEPER/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZOOKEEPER/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZOOKEEPER/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZOOKEEPER/metainfo.xml	(date 1765958707430)
@@ -21,7 +21,7 @@
     <service>
       <name>ZOOKEEPER</name>
       <displayName>ZooKeeper</displayName>
-      <comment>Centralized service which provides highly reliable distributed coordination</comment>
+      <comment>【基础】用于分布式系统的协调与一致性控制，提供选主、配置管理和元数据存储能力，广泛被 Hadoop、HBase、Kafka、Hive 等组件依赖</comment>
       <version>3.5.9</version>
       <components>
 
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/topology.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/topology.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/topology.xml
new file mode 100644
--- /dev/null	(date 1765958702693)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/topology.xml	(date 1765958702693)
@@ -0,0 +1,221 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <!-- topology file -->
+  <property>
+    <name>content</name>
+    <display-name>topology template</display-name>
+    <value>
+        &lt;topology&gt;
+
+            &lt;gateway&gt;
+
+                &lt;provider&gt;
+                    &lt;role&gt;authentication&lt;/role&gt;
+                    &lt;name&gt;ShiroProvider&lt;/name&gt;
+                    &lt;enabled&gt;true&lt;/enabled&gt;
+                    &lt;param&gt;
+                        &lt;name&gt;sessionTimeout&lt;/name&gt;
+                        &lt;value&gt;30&lt;/value&gt;
+                    &lt;/param&gt;
+                    &lt;param&gt;
+                        &lt;name&gt;main.ldapRealm&lt;/name&gt;
+                        &lt;value&gt;org.apache.hadoop.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;
+                    &lt;/param&gt;
+                    &lt;param&gt;
+                        &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;
+                        &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;
+                    &lt;/param&gt;
+                    &lt;param&gt;
+                        &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;
+                        &lt;value&gt;ldap://{{knox_host_name}}:33389&lt;/value&gt;
+                    &lt;/param&gt;
+                    &lt;param&gt;
+                        &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;
+                        &lt;value&gt;simple&lt;/value&gt;
+                    &lt;/param&gt;
+                    &lt;param&gt;
+                        &lt;name&gt;urls./**&lt;/name&gt;
+                        &lt;value&gt;authcBasic&lt;/value&gt;
+                    &lt;/param&gt;
+                &lt;/provider&gt;
+
+                &lt;provider&gt;
+                    &lt;role&gt;identity-assertion&lt;/role&gt;
+                    &lt;name&gt;Default&lt;/name&gt;
+                    &lt;enabled&gt;true&lt;/enabled&gt;
+                &lt;/provider&gt;
+
+                &lt;provider&gt;
+                    &lt;role&gt;authorization&lt;/role&gt;
+                    &lt;name&gt;AclsAuthz&lt;/name&gt;
+                    &lt;enabled&gt;true&lt;/enabled&gt;
+                &lt;/provider&gt;
+
+            &lt;/gateway&gt;
+
+
+            &lt;service&gt;
+                &lt;role&gt;HDFSUI&lt;/role&gt;
+                &lt;url&gt;{{hdfs_ui_url}}&lt;/url&gt;
+            &lt;/service&gt;
+			
+            &lt;service&gt;
+                &lt;role&gt;NAMENODE&lt;/role&gt;
+                &lt;url&gt;{{namenode_address}}&lt;/url&gt;
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;JOBTRACKER&lt;/role&gt;
+                &lt;url&gt;rpc://{{rm_host}}:{{jt_rpc_port}}&lt;/url&gt;
+            &lt;/service&gt;
+
+           &lt;service&gt;
+                &lt;role&gt;YARNUI&lt;/role&gt;
+                &lt;url&gt;http://{{hbase_master_host}}:8088&lt;/url&gt;
+            &lt;/service&gt;
+			
+           &lt;service&gt;
+                &lt;role&gt;YARNUIV2&lt;/role&gt;
+                &lt;url&gt;http://{{hbase_master_host}}:8088&lt;/url&gt;
+            &lt;/service&gt;
+			
+           &lt;service&gt;
+                &lt;role&gt;RESOURCEMANAGER&lt;/role&gt;
+                &lt;url&gt;http://{{hbase_master_host}}:8088/ws&lt;/url&gt;
+            &lt;/service&gt;
+			
+            &lt;service&gt;
+                &lt;role&gt;WEBHDFS&lt;/role&gt;
+                {{webhdfs_service_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;WEBHCAT&lt;/role&gt;
+                &lt;url&gt;http://{{webhcat_server_host}}:{{templeton_port}}/templeton&lt;/url&gt;
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;OOZIE&lt;/role&gt;
+                &lt;url&gt;http://{{oozie_server_host}}:{{oozie_server_port}}/oozie&lt;/url&gt;
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;OOZIEUI&lt;/role&gt;
+                &lt;url&gt;http://{{oozie_server_host}}:{{oozie_server_port}}/oozie/&lt;/url&gt;
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;HBASEUI&lt;/role&gt;
+                &lt;url&gt;http://{{hbase_master_host}}:{{hbase_master_ui_port}}&lt;/url&gt;
+                &lt;version&gt;2.1.0&lt;/version&gt;
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;WEBHBASE&lt;/role&gt;
+                &lt;url&gt;http://{{hbase_master_host}}:{{hbase_master_port}}&lt;/url&gt;
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;HIVE&lt;/role&gt;
+                &lt;url&gt;http://{{hive_server_host}}:{{hive_http_port}}/{{hive_http_path}}&lt;/url&gt;
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;RESOURCEMANAGER&lt;/role&gt;
+                &lt;url&gt;http://{{rm_host}}:{{rm_port}}/ws&lt;/url&gt;
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;DRUID-COORDINATOR-UI&lt;/role&gt;
+                {{druid_coordinator_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;DRUID-COORDINATOR&lt;/role&gt;
+                {{druid_coordinator_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;DRUID-OVERLORD-UI&lt;/role&gt;
+                {{druid_overlord_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;DRUID-OVERLORD&lt;/role&gt;
+                {{druid_overlord_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;DRUID-ROUTER&lt;/role&gt;
+                {{druid_router_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;DRUID-BROKER&lt;/role&gt;
+                {{druid_broker_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;ZEPPELINUI&lt;/role&gt;
+                {{zeppelin_ui_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;ZEPPELINWS&lt;/role&gt;
+                {{zeppelin_ws_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;SPARKHISTORYUI&lt;/role&gt;
+                {{spark_historyserver_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;SOLR&lt;/role&gt;
+                {{solr_server_urls}}
+            &lt;/service&gt;
+
+            &lt;service&gt;
+                &lt;role&gt;TRINOUI&lt;/role&gt;
+                {{trino_urls}}
+            &lt;/service&gt;
+
+        &lt;/topology&gt;
+    </value>
+    <description>
+        The configuration specifies the Hadoop cluster services Knox will provide access to.
+    </description>
+    <value-attributes>
+      <type>content</type>
+      <empty-value-valid>true</empty-value-valid>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-knox-plugin-properties</type>
+        <name>ranger-knox-plugin-enabled</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/users-ldif.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/users-ldif.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/users-ldif.xml
new file mode 100644
--- /dev/null	(date 1765958702701)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/users-ldif.xml	(date 1765958702701)
@@ -0,0 +1,155 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <property>
+    <name>content</name>
+    <display-name>users-ldif template</display-name>
+    <value>
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+version: 1
+
+# Please replace with site specific values
+dn: dc=hadoop,dc=apache,dc=org
+objectclass: organization
+objectclass: dcObject
+o: Hadoop
+dc: hadoop
+
+# Entry for a sample people container
+# Please replace with site specific values
+dn: ou=people,dc=hadoop,dc=apache,dc=org
+objectclass:top
+objectclass:organizationalUnit
+ou: people
+
+# Entry for a sample end user
+# Please replace with site specific values
+dn: uid=guest,ou=people,dc=hadoop,dc=apache,dc=org
+objectclass:top
+objectclass:person
+objectclass:organizationalPerson
+objectclass:inetOrgPerson
+cn: Guest
+sn: User
+uid: guest
+userPassword:guest-password
+modifyTimestamp:20230101080000Z
+
+# entry for sample user admin
+dn: uid=admin,ou=people,dc=hadoop,dc=apache,dc=org
+objectclass:top
+objectclass:person
+objectclass:organizationalPerson
+objectclass:inetOrgPerson
+cn: Admin
+sn: Admin
+uid: admin
+userPassword:admin-password
+modifyTimestamp:20230101080000Z
+
+# entry for sample user sam
+dn: uid=sam,ou=people,dc=hadoop,dc=apache,dc=org
+objectclass:top
+objectclass:person
+objectclass:organizationalPerson
+objectclass:inetOrgPerson
+cn: sam
+sn: sam
+uid: sam
+userPassword:sam-password
+modifyTimestamp:20230101080000Z
+
+# entry for sample user tom
+dn: uid=tom,ou=people,dc=hadoop,dc=apache,dc=org
+objectclass:top
+objectclass:person
+objectclass:organizationalPerson
+objectclass:inetOrgPerson
+cn: tom
+sn: tom
+uid: tom
+userPassword:tom-password
+modifyTimestamp:20230101080000Z
+
+# create FIRST Level groups branch
+dn: ou=groups,dc=hadoop,dc=apache,dc=org
+objectclass:top
+objectclass:organizationalUnit
+ou: groups
+description: generic groups branch
+
+# create the analyst group under groups
+dn: cn=analyst,ou=groups,dc=hadoop,dc=apache,dc=org
+objectclass:top
+objectclass: groupofnames
+cn: analyst
+description:analyst  group
+member: uid=sam,ou=people,dc=hadoop,dc=apache,dc=org
+member: uid=tom,ou=people,dc=hadoop,dc=apache,dc=org
+modifyTimestamp:20230101080000Z
+
+
+# create the scientist group under groups
+dn: cn=scientist,ou=groups,dc=hadoop,dc=apache,dc=org
+objectclass:top
+objectclass: groupofnames
+cn: scientist
+description: scientist group
+member: uid=sam,ou=people,dc=hadoop,dc=apache,dc=org
+modifyTimestamp:20230101080000Z
+
+# create the admin group under groups
+dn: cn=admin,ou=groups,dc=hadoop,dc=apache,dc=org
+objectclass:top
+objectclass: groupofnames
+cn: admin
+description: admin group
+member: uid=admin,ou=people,dc=hadoop,dc=apache,dc=org
+modifyTimestamp:20230101080000Z
+
+        </value>
+    <description>
+            content for users-ldif file for the demo LDAP that comes with Knox.
+        </description>
+    <value-attributes>
+      <type>content</type>
+      <empty-value-valid>true</empty-value-valid>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DOLPHINSCHEDULER/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DOLPHINSCHEDULER/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DOLPHINSCHEDULER/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DOLPHINSCHEDULER/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/DOLPHINSCHEDULER/metainfo.xml	(date 1765958707823)
@@ -19,9 +19,7 @@
         <service>
             <name>DOLPHINSCHEDULER</name>
             <displayName>Dolphin Scheduler</displayName>
-            <comment>
-                Component DolphinScheduler Power By JaneTTR . mail: 3832514048@qq.com ,git: https://gitee.com/tt-bigdata/ambari-env
-            </comment>
+            <comment>【增强】分布式工作流调度与任务编排服务，用于调度 Spark、Flink、Shell 等任务，使用前需安装并部署数据库、ZooKeeper 等依赖组件</comment>
             <version>3.2.2</version>
             <components>
                 <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-security.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-security.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-security.xml
new file mode 100644
--- /dev/null	(date 1765958702736)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-security.xml	(date 1765958702736)
@@ -0,0 +1,64 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>ranger.plugin.knox.service.name</name>
+    <value>{{repo_name}}</value>
+    <description>Name of the Ranger service containing policies for this Knox instance</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.knox.policy.source.impl</name>
+    <value>org.apache.ranger.admin.client.RangerAdminJersey2RESTClient</value>
+    <description>Class to retrieve policies from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.knox.policy.rest.url</name>
+    <value>{{policymgr_mgr_url}}</value>
+    <description>URL to Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+    <depends-on>
+      <property>
+        <type>admin-properties</type>
+        <name>policymgr_external_url</name>
+      </property>
+    </depends-on>
+  </property>
+  <property>
+    <name>ranger.plugin.knox.policy.rest.ssl.config.file</name>
+    <value>/usr/hdp/current/knox-server/conf/ranger-policymgr-ssl.xml</value>
+    <description>Path to the file containing SSL details to contact Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.knox.policy.pollIntervalMs</name>
+    <value>30000</value>
+    <description>How often to poll for changes in policies?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.knox.policy.cache.dir</name>
+    <value>/etc/ranger/{{repo_name}}/policycache</value>
+    <description>Directory where Ranger policies are cached after successful retrieval from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-log4j.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-log4j.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-log4j.xml
new file mode 100644
--- /dev/null	(date 1765958702714)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-log4j.xml	(date 1765958702714)
@@ -0,0 +1,110 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="false">
+   <property>
+    <name>knox_gateway_log_maxfilesize</name>
+    <value>256</value>
+    <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>Knox Gateway Log: backup file size</display-name>
+   <value-attributes>
+      <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>knox_gateway_log_maxbackupindex</name>
+    <value>20</value>
+    <description>The number of backup files</description>
+    <display-name>Knox Gateway Log: # of backup files</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>gateway-log4j template</display-name>
+    <value>
+
+      # Licensed to the Apache Software Foundation (ASF) under one
+      # or more contributor license agreements. See the NOTICE file
+      # distributed with this work for additional information
+      # regarding copyright ownership. The ASF licenses this file
+      # to you under the Apache License, Version 2.0 (the
+      # "License"); you may not use this file except in compliance
+      # with the License. You may obtain a copy of the License at
+      #
+      # http://www.apache.org/licenses/LICENSE-2.0
+      #
+      # Unless required by applicable law or agreed to in writing, software
+      # distributed under the License is distributed on an "AS IS" BASIS,
+      # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+      # See the License for the specific language governing permissions and
+      # limitations under the License.
+
+      app.log.dir=${launcher.dir}/../logs
+      app.log.file=${launcher.name}.log
+      app.audit.file=${launcher.name}-audit.log
+
+      log4j.rootLogger=ERROR, drfa
+
+      log4j.logger.org.apache.knox.gateway=INFO
+      #log4j.logger.org.apache.knox.gateway=DEBUG
+
+      #log4j.logger.org.eclipse.jetty=DEBUG
+      #log4j.logger.org.apache.shiro=DEBUG
+      #log4j.logger.org.apache.http=DEBUG
+      #log4j.logger.org.apache.http.client=DEBUG
+      #log4j.logger.org.apache.http.headers=DEBUG
+      #log4j.logger.org.apache.http.wire=DEBUG
+
+      log4j.appender.stdout=org.apache.log4j.ConsoleAppender
+      log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
+      log4j.appender.stdout.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
+
+      log4j.appender.drfa=org.apache.log4j.DailyRollingFileAppender
+      log4j.appender.drfa.File=${app.log.dir}/${app.log.file}
+      log4j.appender.drfa.DatePattern=.yyyy-MM-dd
+      log4j.appender.drfa.layout=org.apache.log4j.PatternLayout
+      log4j.appender.drfa.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+      log4j.appender.drfa.MaxFileSize = {{knox_gateway_log_maxfilesize}}MB
+      log4j.appender.drfa.MaxBackupIndex = {{knox_gateway_log_maxbackupindex}}
+
+      log4j.logger.audit=INFO, auditfile
+      log4j.appender.auditfile=org.apache.log4j.DailyRollingFileAppender
+      log4j.appender.auditfile.File=${app.log.dir}/${app.audit.file}
+      log4j.appender.auditfile.Append = true
+      log4j.appender.auditfile.DatePattern = '.'yyyy-MM-dd
+      log4j.appender.auditfile.layout = org.apache.hadoop.gateway.audit.log4j.layout.AuditLayout
+
+    </value>
+    <description>
+      content for log4j.properties file for Knox.
+    </description>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hadoop-env.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hadoop-env.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hadoop-env.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hadoop-env.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hadoop-env.xml	(date 1765958700111)
@@ -20,229 +20,233 @@
  */
 -->
 <configuration supports_adding_forbidden="true">
-  <property>
-    <name>hdfs_log_dir_prefix</name>
-    <value>/var/log/hadoop</value>
-    <description>Hadoop Log Dir Prefix</description>
-    <display-name>Hadoop Log Dir Prefix</display-name>
-    <value-attributes>
-      <type>directory</type>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>hadoop_pid_dir_prefix</name>
-    <value>/var/run/hadoop</value>
-    <display-name>Hadoop PID Dir Prefix</display-name>
-    <description>Hadoop PID Dir Prefix</description>
-    <value-attributes>
-      <type>directory</type>
-      <overridable>false</overridable>
-      <editable-only-at-install>true</editable-only-at-install>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>hadoop_root_logger</name>
-    <value>INFO,RFA</value>
-    <display-name>Hadoop Root Logger</display-name>
-    <description>Hadoop Root Logger</description>
-    <value-attributes>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>hadoop_heapsize</name>
-    <value>1024</value>
-    <description>Hadoop maximum Java heap size</description>
-    <display-name>Hadoop maximum Java heap size</display-name>
-    <value-attributes>
-      <type>int</type>
-      <unit>MB</unit>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>namenode_heapsize</name>
-    <value>1024</value>
-    <description>NameNode Java heap size</description>
-    <display-name>NameNode Java heap size</display-name>
-    <value-attributes>
-      <type>int</type>
-      <minimum>0</minimum>
-      <maximum>268435456</maximum>
-      <unit>MB</unit>
-      <increment-step>256</increment-step>
-      <overridable>false</overridable>
-    </value-attributes>
-    <depends-on>
-      <property>
-        <type>hdfs-site</type>
-        <name>dfs.datanode.data.dir</name>
-      </property>
-    </depends-on>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>namenode_opt_newsize</name>
-    <value>200</value>
-    <description>Default size of Java new generation for NameNode (Java option -XX:NewSize) Note: The value of NameNode new generation size (default size of Java new generation for NameNode (Java option -XX:NewSize)) should be 1/8 of maximum heap size (-Xmx). Ensure that the value of the namenode_opt_newsize property is 1/8 the value of maximum heap size (-Xmx).</description>
-    <display-name>NameNode new generation size</display-name>
-    <depends-on>
-      <property>
-        <type>hadoop-env</type>
-        <name>namenode_heapsize</name>
-      </property>
-    </depends-on>
-    <value-attributes>
-      <type>int</type>
-      <minimum>0</minimum>
-      <maximum>16384</maximum>
-      <unit>MB</unit>
-      <increment-step>256</increment-step>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>namenode_opt_maxnewsize</name>
-    <value>200</value>
-    <description>NameNode maximum new generation size</description>
-    <display-name>NameNode maximum new generation size</display-name>
-    <depends-on>
-      <property>
-        <type>hadoop-env</type>
-        <name>namenode_heapsize</name>
-      </property>
-    </depends-on>
-    <value-attributes>
-      <type>int</type>
-      <minimum>0</minimum>
-      <maximum>16384</maximum>
-      <unit>MB</unit>
-      <increment-step>256</increment-step>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>namenode_opt_permsize</name>
-    <value>128</value>
-    <description>NameNode permanent generation size</description>
-    <display-name>NameNode permanent generation size</display-name>
-    <value-attributes>
-      <type>int</type>
-      <minimum>0</minimum>
-      <maximum>2096</maximum>
-      <unit>MB</unit>
-      <increment-step>128</increment-step>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>namenode_opt_maxpermsize</name>
-    <value>256</value>
-    <description>NameNode maximum permanent generation size</description>
-    <display-name>NameNode maximum permanent generation size</display-name>
-    <value-attributes>
-      <type>int</type>
-      <minimum>0</minimum>
-      <maximum>2096</maximum>
-      <unit>MB</unit>
-      <increment-step>128</increment-step>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dtnode_heapsize</name>
-    <value>1024</value>
-    <description>DataNode maximum Java heap size</description>
-    <display-name>DataNode maximum Java heap size</display-name>
-    <value-attributes>
-      <type>int</type>
-      <minimum>0</minimum>
-      <maximum>268435456</maximum>
-      <unit>MB</unit>
-      <increment-step>128</increment-step>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>hdfs_user</name>
-    <display-name>HDFS User</display-name>
-    <value>hdfs</value>
-    <property-type>USER</property-type>
-    <description>User to run HDFS as</description>
-    <value-attributes>
-      <type>user</type>
-      <overridable>false</overridable>
-      <user-groups>
-        <property>
-          <type>cluster-env</type>
-          <name>user_group</name>
-        </property>
-        <property>
-          <type>hdfs-site</type>
-          <name>dfs.permissions.superusergroup</name>
-        </property>
-      </user-groups>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>hdfs_tmp_dir</name>
-    <value>/tmp</value>
-    <description>HDFS tmp Dir</description>
-    <display-name>HDFS tmp Dir</display-name>
-    <property-type>NOT_MANAGED_HDFS_PATH</property-type>
-    <value-attributes>
-      <read-only>true</read-only>
-      <overridable>false</overridable>
-      <visible>false</visible>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>hdfs_user_nofile_limit</name>
-    <value>128000</value>
-    <description>Max open files limit setting for HDFS user.</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>hdfs_user_nproc_limit</name>
-    <value>65536</value>
-    <description>Max number of processes limit setting for HDFS user.</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>namenode_backup_dir</name>
-    <display-name>NameNode Backup directory</display-name>
-    <description>Local directory for storing backup copy of NameNode images during upgrade</description>
-    <value>/tmp/upgrades</value>
-    <on-ambari-upgrade add="false"/>
-  </property>
-  <property>
-    <name>hdfs_user_keytab</name>
-    <description>HDFS keytab path</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>hdfs_principal_name</name>
-    <description>HDFS principal name</description>
-    <property-type>KERBEROS_PRINCIPAL</property-type>
-    <on-ambari-upgrade add="true"/>
-  </property>
+    <property>
+        <name>hdfs_log_dir_prefix</name>
+        <value>/var/log/hadoop</value>
+        <description>Hadoop Log Dir Prefix</description>
+        <display-name>Hadoop Log Dir Prefix</display-name>
+        <value-attributes>
+            <type>directory</type>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>hadoop_pid_dir_prefix</name>
+        <value>/var/run/hadoop</value>
+        <display-name>Hadoop PID Dir Prefix</display-name>
+        <description>Hadoop PID Dir Prefix</description>
+        <value-attributes>
+            <type>directory</type>
+            <overridable>false</overridable>
+            <editable-only-at-install>true</editable-only-at-install>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>hadoop_root_logger</name>
+        <value>INFO,RFA</value>
+        <display-name>Hadoop Root Logger</display-name>
+        <description>Hadoop Root Logger</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>hadoop_heapsize</name>
+        <value>1024</value>
+        <description>Hadoop maximum Java heap size</description>
+        <display-name>Hadoop maximum Java heap size</display-name>
+        <value-attributes>
+            <type>int</type>
+            <unit>MB</unit>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>namenode_heapsize</name>
+        <value>1024</value>
+        <description>NameNode Java heap size</description>
+        <display-name>NameNode Java heap size</display-name>
+        <value-attributes>
+            <type>int</type>
+            <minimum>0</minimum>
+            <maximum>268435456</maximum>
+            <unit>MB</unit>
+            <increment-step>256</increment-step>
+            <overridable>false</overridable>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>hdfs-site</type>
+                <name>dfs.datanode.data.dir</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>namenode_opt_newsize</name>
+        <value>200</value>
+        <description>Default size of Java new generation for NameNode (Java option -XX:NewSize) Note: The value of
+            NameNode new generation size (default size of Java new generation for NameNode (Java option -XX:NewSize))
+            should be 1/8 of maximum heap size (-Xmx). Ensure that the value of the namenode_opt_newsize property is 1/8
+            the value of maximum heap size (-Xmx).
+        </description>
+        <display-name>NameNode new generation size</display-name>
+        <depends-on>
+            <property>
+                <type>hadoop-env</type>
+                <name>namenode_heapsize</name>
+            </property>
+        </depends-on>
+        <value-attributes>
+            <type>int</type>
+            <minimum>0</minimum>
+            <maximum>16384</maximum>
+            <unit>MB</unit>
+            <increment-step>256</increment-step>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>namenode_opt_maxnewsize</name>
+        <value>200</value>
+        <description>NameNode maximum new generation size</description>
+        <display-name>NameNode maximum new generation size</display-name>
+        <depends-on>
+            <property>
+                <type>hadoop-env</type>
+                <name>namenode_heapsize</name>
+            </property>
+        </depends-on>
+        <value-attributes>
+            <type>int</type>
+            <minimum>0</minimum>
+            <maximum>16384</maximum>
+            <unit>MB</unit>
+            <increment-step>256</increment-step>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>namenode_opt_permsize</name>
+        <value>128</value>
+        <description>NameNode permanent generation size</description>
+        <display-name>NameNode permanent generation size</display-name>
+        <value-attributes>
+            <type>int</type>
+            <minimum>0</minimum>
+            <maximum>2096</maximum>
+            <unit>MB</unit>
+            <increment-step>128</increment-step>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>namenode_opt_maxpermsize</name>
+        <value>256</value>
+        <description>NameNode maximum permanent generation size</description>
+        <display-name>NameNode maximum permanent generation size</display-name>
+        <value-attributes>
+            <type>int</type>
+            <minimum>0</minimum>
+            <maximum>2096</maximum>
+            <unit>MB</unit>
+            <increment-step>128</increment-step>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dtnode_heapsize</name>
+        <value>1024</value>
+        <description>DataNode maximum Java heap size</description>
+        <display-name>DataNode maximum Java heap size</display-name>
+        <value-attributes>
+            <type>int</type>
+            <minimum>0</minimum>
+            <maximum>268435456</maximum>
+            <unit>MB</unit>
+            <increment-step>128</increment-step>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>hdfs_user</name>
+        <display-name>HDFS User</display-name>
+        <value>hdfs</value>
+        <property-type>USER</property-type>
+        <description>User to run HDFS as</description>
+        <value-attributes>
+            <type>user</type>
+            <overridable>false</overridable>
+            <user-groups>
+                <property>
+                    <type>cluster-env</type>
+                    <name>user_group</name>
+                </property>
+                <property>
+                    <type>hdfs-site</type>
+                    <name>dfs.permissions.superusergroup</name>
+                </property>
+            </user-groups>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>hdfs_tmp_dir</name>
+        <value>/tmp</value>
+        <description>HDFS tmp Dir</description>
+        <display-name>HDFS tmp Dir</display-name>
+        <property-type>NOT_MANAGED_HDFS_PATH</property-type>
+        <value-attributes>
+            <read-only>true</read-only>
+            <overridable>false</overridable>
+            <visible>false</visible>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>hdfs_user_nofile_limit</name>
+        <value>128000</value>
+        <description>Max open files limit setting for HDFS user.</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>hdfs_user_nproc_limit</name>
+        <value>65536</value>
+        <description>Max number of processes limit setting for HDFS user.</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>namenode_backup_dir</name>
+        <display-name>NameNode Backup directory</display-name>
+        <description>Local directory for storing backup copy of NameNode images during upgrade</description>
+        <value>/tmp/upgrades</value>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>hdfs_user_keytab</name>
+        <description>HDFS keytab path</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>hdfs_principal_name</name>
+        <description>HDFS principal name</description>
+        <property-type>KERBEROS_PRINCIPAL</property-type>
+        <on-ambari-upgrade add="true"/>
+    </property>
   <!-- hadoop-env.sh -->
-  <property>
-    <name>content</name>
-    <display-name>hadoop-env template</display-name>
-    <description>This is the jinja template for hadoop-env.sh file</description>
-    <value>
+    <property>
+        <name>content</name>
+        <display-name>hadoop-env template</display-name>
+        <description>This is the jinja template for hadoop-env.sh file</description>
+        <value>
 # Set Hadoop-specific environment variables here.
 
 # The only required environment variable is JAVA_HOME.  All others are
@@ -377,16 +381,28 @@
 export JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:{{hadoop_lib_home}}/native
 
 {% if is_datanode_max_locked_memory_set %}
-# Fix temporary bug, when ulimit from conf files is not picked up, without full relogin. 
-# Makes sense to fix only when runing DN as root 
+# Fix temporary bug, when ulimit from conf files is not picked up, without full relogin.
+# Makes sense to fix only when runing DN as root
 if [ "$command" == "datanode" ] &amp;&amp; [ "$EUID" -eq 0 ] &amp;&amp; [ -n "$HADOOP_SECURE_DN_USER" ]; then
   ulimit -l {{datanode_max_locked_memory}}
 fi
 {% endif %}
-    </value>
-    <value-attributes>
-      <type>content</type>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
+        </value>
+        <value-attributes>
+            <type>content</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>proxyuser_group</name>
+        <value>hadoop</value>
+        <display-name>proxyuser_group</display-name>
+        <description>用于 Hadoop 代理用户（proxyuser）规则的组名，例如 hadoop.proxyuser.knox.groups 会引用该值。
+        </description>
+        <value-attributes>
+            <type>string</type>
+            <overridable>true</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
 </configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ldap-log4j2.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ldap-log4j2.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ldap-log4j2.xml
new file mode 100644
--- /dev/null	(date 1765958702704)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ldap-log4j2.xml	(date 1765958702704)
@@ -0,0 +1,73 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="true">
+    <property>
+        <name>content</name>
+        <description>gateway-log4j2.xml</description>
+        <value><![CDATA[<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<Configuration>
+    <Properties>
+        <Property name="app.log.dir">${sys:launcher.dir}/../logs</Property>
+        <Property name="app.log.file">${sys:launcher.name}.log</Property>
+    </Properties>
+    <Appenders>
+        <Console name="stdout" target="SYSTEM_OUT">
+            <PatternLayout pattern="%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n" />
+        </Console>
+        <RollingFile name="drfa" fileName="${app.log.dir}/${app.log.file}" filePattern="${app.log.dir}/${app.log.file}.%d{yyyy-MM-dd}">
+            <PatternLayout pattern="%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n" />
+            <TimeBasedTriggeringPolicy />
+        </RollingFile>
+    </Appenders>
+    <Loggers>
+        <Logger name="org.apache.directory" level="WARN" />
+        <Logger name="org.apache.directory.server.ldap.LdapServer" level="INFO" />
+        <Root level="ERROR">
+            <AppenderRef ref="drfa" />
+        </Root>
+    </Loggers>
+</Configuration>
+
+]]>
+        </value>
+        <value-attributes>
+            <type>content</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HIVE/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HIVE/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HIVE/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HIVE/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HIVE/metainfo.xml	(date 1765958701662)
@@ -21,9 +21,7 @@
     <service>
       <name>HIVE</name>
       <displayName>Hive</displayName>
-      <comment>Data warehouse system for ad-hoc queries &amp; analysis of large datasets and table &amp; storage
-            management service
-          </comment>
+      <comment>【基础】基于 SQL 的数据仓库服务，用于对 HDFS 等存储上的数据进行查询与分析，使用前需安装并部署 HDFS、Metastore 数据库及相关计算引擎</comment>
       <version>3.1.3</version>
       <credential-store>
         <supported>true</supported>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-log4j2.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-log4j2.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-log4j2.xml
new file mode 100644
--- /dev/null	(date 1765958702720)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/gateway-log4j2.xml	(date 1765958702720)
@@ -0,0 +1,117 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="true">
+    <property>
+        <name>content</name>
+        <description>gateway-log4j2.xml</description>
+        <value><![CDATA[<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<Configuration>
+    <Properties>
+        <Property name="app.log.dir">${sys:launcher.dir}/../logs</Property>
+        <Property name="app.log.file">${sys:launcher.name}.log</Property>
+        <Property name="app.audit.file">${sys:launcher.name}-audit.log</Property>
+    </Properties>
+
+    <Appenders>
+        <RollingFile name="auditfile" fileName="${app.log.dir}/${app.audit.file}" filePattern="${app.log.dir}/${app.audit.file}.%d{yyyy-MM-dd}">
+            <AuditLayout />
+            <TimeBasedTriggeringPolicy />
+        </RollingFile>
+        <Console name="stdout" target="SYSTEM_OUT">
+            <PatternLayout pattern="%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n" />
+        </Console>
+        <RollingFile name="drfa" fileName="${app.log.dir}/${app.log.file}" filePattern="${app.log.dir}/${app.log.file}.%d{yyyy-MM-dd}">
+            <!-- Same as ISO8601 format but without the 'T' (log4j1 compatible) -->
+            <PatternLayout pattern="%d{yyyy-MM-dd' 'HH:mm:ss,SSS} %X{trace_id} %-5p %c{2} (%F:%M(%L)) - %m%n" />
+            <TimeBasedTriggeringPolicy />
+        </RollingFile>
+<!--        <RollingFile name="httpclient" fileName="${app.log.dir}/${launcher.name}-http-client.log" filePattern="${app.log.dir}/${launcher.name}-http-client.log.%d{yyyy-MM-dd}">-->
+<!--            <PatternLayout pattern="%d{ISO8601}|%t|%m%n" />-->
+<!--            <TimeBasedTriggeringPolicy />-->
+<!--        </RollingFile>-->
+<!--        <RollingFile name="httpaccess" fileName="${app.log.dir}/${launcher.name}-http-access.log" filePattern="${app.log.dir}/${launcher.name}-http-access.log.%d{yyyy-MM-dd}">-->
+<!--            <PatternLayout pattern="%d{ISO8601}|%t|%m%n" />-->
+<!--            <TimeBasedTriggeringPolicy />-->
+<!--        </RollingFile>-->
+<!--        <RollingFile name="httpserver" fileName="${app.log.dir}/${launcher.name}-http-server.log" filePattern="${app.log.dir}/${launcher.name}-http-server.log.%d{yyyy-MM-dd}">-->
+<!--            <PatternLayout pattern="%d{ISO8601}|%t|%m%n" />-->
+<!--            <TimeBasedTriggeringPolicy />-->
+<!--        </RollingFile>-->
+    </Appenders>
+    <Loggers>
+        <Logger name="audit" level="INFO">
+            <AppenderRef ref="auditfile" />
+        </Logger>
+        <Logger name="org.apache.knox.gateway" level="INFO" />
+        <Root level="ERROR">
+            <AppenderRef ref="drfa" />
+        </Root>
+<!--        <Logger name="org.apache.knox.gateway.websockets" level="DEBUG" />-->
+<!--        <Logger name="org.springframework" level="DEBUG" />-->
+<!--        <Logger name="org.apache.knox.gateway.http.request.body" level="OFF" />-->
+<!--        <Logger name="org.apache.knox.gateway.http" level="TRACE">-->
+<!--            <AppenderRef ref="httpserver" />-->
+<!--        </Logger>-->
+<!--        <Logger name="org.apache.shiro" level="DEBUG" />-->
+<!--        <Logger name="org.apache.knox.gateway.http.response.body" level="OFF" />-->
+<!--        <Logger name="org.apache.http.client" level="DEBUG" />-->
+<!--        <Logger name="org.apache.knox.gateway.http.request.headers" level="OFF" />-->
+<!--        <Logger name="org.apache.http.wire" level="DEBUG">-->
+<!--            <AppenderRef ref="httpclient" />-->
+<!--        </Logger>-->
+<!--        <Logger name="org.apache.knox.gateway.http.response.headers" level="OFF" />-->
+<!--        <Logger name="net.sf.ehcache" level="DEBUG" />-->
+<!--        <Logger name="org.apache.http" level="DEBUG" />-->
+<!--        <Logger name="org.apache.http.headers" level="DEBUG" />-->
+<!--        <Logger name="org.apache.shiro.util.ThreadContext" level="DEBUG" />-->
+<!--        <Logger name="org.apache.knox.gateway" level="DEBUG" />-->
+<!--        <Logger name="org.eclipse.jetty" level="DEBUG" />-->
+<!--        <Logger name="org.apache.knox.gateway.access" level="TRACE">-->
+<!--            <AppenderRef ref="httpaccess" />-->
+<!--        </Logger>-->
+    </Loggers>
+</Configuration>
+
+]]>
+        </value>
+        <value-attributes>
+            <type>content</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knoxcli-log4j2.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knoxcli-log4j2.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knoxcli-log4j2.xml
new file mode 100644
--- /dev/null	(date 1765958702723)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knoxcli-log4j2.xml	(date 1765958702723)
@@ -0,0 +1,71 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="true">
+    <property>
+        <name>content</name>
+        <description>gateway-log4j2.xml</description>
+        <value><![CDATA[<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<Configuration>
+    <Properties>
+        <Property name="app.log.dir">${sys:launcher.dir}/../logs</Property>
+        <Property name="app.log.file">${sys:launcher.name}.log</Property>
+    </Properties>
+    <Appenders>
+        <Console name="stdout" target="SYSTEM_OUT">
+            <PatternLayout pattern="%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n" />
+        </Console>
+        <RollingFile name="drfa" fileName="${app.log.dir}/${app.log.file}" filePattern="${app.log.dir}/${app.log.file}.%d{yyyy-MM-dd}">
+            <PatternLayout pattern="%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n" />
+            <TimeBasedTriggeringPolicy />
+        </RollingFile>
+    </Appenders>
+    <Loggers>
+        <Logger name="org.apache.knox.gateway" level="INFO" />
+        <Root level="ERROR">
+            <AppenderRef ref="drfa" />
+        </Root>
+    </Loggers>
+</Configuration>
+]]>
+        </value>
+        <value-attributes>
+            <type>content</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-policymgr-ssl.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-policymgr-ssl.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-policymgr-ssl.xml
new file mode 100644
--- /dev/null	(date 1765958703028)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-policymgr-ssl.xml	(date 1765958703028)
@@ -0,0 +1,72 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore</name>
+    <value/>
+    <description>Java Keystore files</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.password</name>
+    <value>myKeyFilePassword</value>
+    <property-type>PASSWORD</property-type>
+    <description>password for keystore</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore</name>
+    <value/>
+    <description>java truststore file</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.password</name>
+    <value>changeit</value>
+    <property-type>PASSWORD</property-type>
+    <description>java truststore password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.credential.file</name>
+    <value>jceks://file{{credential_file}}</value>
+    <description>java keystore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.credential.file</name>
+    <value>jceks://file{{credential_file}}</value>
+    <description>java truststore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-plugin-properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-plugin-properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-plugin-properties.xml
new file mode 100644
--- /dev/null	(date 1765958703034)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-plugin-properties.xml	(date 1765958703034)
@@ -0,0 +1,132 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="true">
+  <property>
+    <name>policy_user</name>
+    <value>ambari-qa</value>
+    <display-name>Policy user for KNOX</display-name>
+    <description>This user must be system user and also present at Ranger admin portal</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>common.name.for.certificate</name>
+    <value/>
+    <description>Common name for certificate, this value should match what is specified in repo within ranger admin</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-knox-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Enable Ranger for KNOX</display-name>
+    <description>Enable ranger knox plugin ?</description>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>ranger-knox-plugin-enabled</name>
+      </property>
+    </depends-on>
+    <value-attributes>
+      <type>boolean</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>REPOSITORY_CONFIG_USERNAME</name>
+    <value>admin</value>
+    <display-name>Ranger repository config user</display-name>
+    <description>Used for repository creation on ranger admin</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>REPOSITORY_CONFIG_PASSWORD</name>
+    <value>admin-password</value>
+    <property-type>PASSWORD</property-type>
+    <display-name>Ranger repository config password</display-name>
+    <description>Used for repository creation on ranger admin</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>KNOX_HOME</name>
+    <value>/usr/hdp/current/knox-server</value>
+    <display-name>Knox Home</display-name>
+    <description>Knox home folder</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+
+
+  <property>
+    <name>external_admin_username</name>
+    <value></value>
+    <display-name>External Ranger admin username</display-name>
+    <description>Add ranger default admin username if want to communicate to external ranger</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_admin_password</name>
+    <value></value>
+    <display-name>External Ranger admin password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Add ranger default admin password if want to communicate to external ranger</description>
+    <value-attributes>
+      <type>password</type>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_ranger_admin_username</name>
+    <value></value>
+    <display-name>External Ranger Ambari admin username</display-name>
+    <description>Add ranger default ambari admin username if want to communicate to external ranger</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_ranger_admin_password</name>
+    <value></value>
+    <display-name>External Ranger Ambari admin password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Add ranger default ambari admin password if want to communicate to external ranger</description>
+    <value-attributes>
+      <type>password</type>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knoxsso-topology.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knoxsso-topology.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knoxsso-topology.xml
new file mode 100644
--- /dev/null	(date 1765958702728)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/knoxsso-topology.xml	(date 1765958702728)
@@ -0,0 +1,122 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <!-- topology file -->
+    <property>
+        <name>content</name>
+        <display-name>knoxsso-topology template</display-name>
+        <value>
+            &lt;topology&gt;
+            &lt;gateway&gt;
+            &lt;provider&gt;
+            &lt;role&gt;webappsec&lt;/role&gt;
+            &lt;name&gt;WebAppSec&lt;/name&gt;
+            &lt;enabled&gt;true&lt;/enabled&gt;
+            &lt;param&gt;&lt;name&gt;xframe.options.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/param&gt;
+            &lt;/provider&gt;
+
+            &lt;provider&gt;
+            &lt;role&gt;authentication&lt;/role&gt;
+            &lt;name&gt;ShiroProvider&lt;/name&gt;
+            &lt;enabled&gt;true&lt;/enabled&gt;
+            &lt;param&gt;
+            &lt;name&gt;sessionTimeout&lt;/name&gt;
+            &lt;value&gt;30&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;redirectToUrl&lt;/name&gt;
+            &lt;value&gt;/gateway/knoxsso/knoxauth/login.html&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;restrictedCookies&lt;/name&gt;
+            &lt;value&gt;rememberme,WWW-Authenticate&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;main.ldapRealm&lt;/name&gt;
+            &lt;value&gt;org.apache.hadoop.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;main.ldapContextFactory&lt;/name&gt;
+            &lt;value&gt;org.apache.hadoop.gateway.shirorealm.KnoxLdapContextFactory&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;main.ldapRealm.contextFactory&lt;/name&gt;
+            &lt;value&gt;$ldapContextFactory&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;
+            &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;
+            &lt;value&gt;ldap://localhost:33389&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;main.ldapRealm.authenticationCachingEnabled&lt;/name&gt;
+            &lt;value&gt;false&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;
+            &lt;value&gt;simple&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;urls./**&lt;/name&gt;
+            &lt;value&gt;authcBasic&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;/provider&gt;
+
+            &lt;provider&gt;
+            &lt;role&gt;identity-assertion&lt;/role&gt;
+            &lt;name&gt;Default&lt;/name&gt;
+            &lt;enabled&gt;true&lt;/enabled&gt;
+            &lt;/provider&gt;
+            &lt;/gateway&gt;
+
+            &lt;application&gt;
+            &lt;name&gt;knoxauth&lt;/name&gt;
+            &lt;/application&gt;
+
+            &lt;service&gt;
+            &lt;role&gt;KNOXSSO&lt;/role&gt;
+            &lt;param&gt;
+            &lt;name&gt;knoxsso.cookie.secure.only&lt;/name&gt;
+            &lt;value&gt;false&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;param&gt;
+            &lt;name&gt;knoxsso.token.ttl&lt;/name&gt;
+            &lt;value&gt;30000&lt;/value&gt;
+            &lt;/param&gt;
+            &lt;/service&gt;
+
+            &lt;/topology&gt;
+        </value>
+        <description>
+            The configuration specifies the KnoxSSO provider integration, cookie and token management details.
+        </description>
+        <value-attributes>
+            <type>content</type>
+            <empty-value-valid>true</empty-value-valid>
+            <show-property-name>false</show-property-name>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/admin-topology.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/admin-topology.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/admin-topology.xml
new file mode 100644
--- /dev/null	(date 1765958702717)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/admin-topology.xml	(date 1765958702717)
@@ -0,0 +1,105 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <!-- topology file -->
+  <property>
+    <name>content</name>
+    <display-name>admin-topology template</display-name>
+    <value>
+    &lt;topology&gt;
+
+        &lt;gateway&gt;
+
+             &lt;provider&gt;
+                &lt;role&gt;authentication&lt;/role&gt;
+                &lt;name&gt;ShiroProvider&lt;/name&gt;
+                &lt;enabled&gt;true&lt;/enabled&gt;
+                &lt;param&gt;
+                    &lt;name&gt;sessionTimeout&lt;/name&gt;
+                    &lt;value&gt;30&lt;/value&gt;
+                &lt;/param&gt;
+                &lt;param&gt;
+                    &lt;name&gt;main.ldapRealm&lt;/name&gt;
+                    &lt;value&gt;org.apache.hadoop.gateway.shirorealm.KnoxLdapRealm&lt;/value&gt;
+                &lt;/param&gt;
+                &lt;param&gt;
+                    &lt;name&gt;main.ldapRealm.userDnTemplate&lt;/name&gt;
+                    &lt;value&gt;uid={0},ou=people,dc=hadoop,dc=apache,dc=org&lt;/value&gt;
+                &lt;/param&gt;
+                &lt;param&gt;
+                    &lt;name&gt;main.ldapRealm.contextFactory.url&lt;/name&gt;
+                    &lt;value&gt;ldap://{{knox_host_name}}:33389&lt;/value&gt;
+                &lt;/param&gt;
+                &lt;param&gt;
+                    &lt;name&gt;main.ldapRealm.contextFactory.authenticationMechanism&lt;/name&gt;
+                    &lt;value&gt;simple&lt;/value&gt;
+                &lt;/param&gt;
+                &lt;param&gt;
+                    &lt;name&gt;urls./**&lt;/name&gt;
+                    &lt;value&gt;authcBasic&lt;/value&gt;
+                &lt;/param&gt;
+            &lt;/provider&gt;
+
+            &lt;provider&gt;
+                &lt;role&gt;authorization&lt;/role&gt;
+                &lt;name&gt;AclsAuthz&lt;/name&gt;
+                &lt;enabled&gt;true&lt;/enabled&gt;
+                &lt;param&gt;
+	               &lt;name>knox.acl.mode&lt;/name&gt;
+	               &lt;value&gt;OR&lt;/value&gt;
+                   &lt;/param&gt;
+                &lt;param&gt;
+                    &lt;name&gt;knox.acl&lt;/name&gt;
+                    &lt;value&gt;KNOX_ADMIN_USERS;KNOX_ADMIN_GROUPS;*&lt;/value&gt;
+                &lt;/param&gt;
+            &lt;/provider&gt;
+
+            &lt;provider&gt;
+                &lt;role&gt;identity-assertion&lt;/role&gt;
+                &lt;name&gt;HadoopGroupProvider&lt;/name&gt;
+                &lt;enabled&gt;true&lt;/enabled&gt;
+                &lt;param&gt;
+                    &lt;name&gt;CENTRAL_GROUP_CONFIG_PREFIX&lt;/name&gt;
+                    &lt;value&gt;gateway.group.config.&lt;/value&gt;
+                &lt;/param&gt;
+            &lt;/provider&gt;
+
+        &lt;/gateway&gt;
+
+        &lt;service&gt;
+            &lt;role&gt;KNOX&lt;/role&gt;
+        &lt;/service&gt;
+
+    &lt;/topology&gt;
+
+    </value>
+    <description>
+        The configuration specifies the Knox admin API configuration and access details. The authentication provider should be configured to match your deployment details.
+    </description>
+    <value-attributes>
+      <type>content</type>
+      <empty-value-valid>true</empty-value-valid>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUE/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUE/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUE/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUE/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HUE/metainfo.xml	(date 1765958699323)
@@ -21,7 +21,7 @@
     <service>
       <name>HUE</name>
       <displayName>Hue</displayName>
-      <comment>Hue is a graphical user interface to operate and develop applications for Apache Hadoop.</comment>
+      <comment>【增强】基于 Web 的大数据交互与管理界面，用于统一访问和管理 Hive、HDFS 等服务，使用前需安装并部署相应的大数据组件及认证服务</comment>
       <version>4.11.0</version>
       <selection>TECH_PREVIEW</selection>
       <components>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-audit.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-audit.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-audit.xml
new file mode 100644
--- /dev/null	(date 1765958702732)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/ranger-knox-audit.xml	(date 1765958702732)
@@ -0,0 +1,132 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>xasecure.audit.is.enabled</name>
+    <value>true</value>
+    <description>Is Audit enabled?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs</name>
+    <value>true</value>
+    <display-name>Audit to HDFS</display-name>
+    <description>Is Audit to HDFS enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.hdfs</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.dir</name>
+    <value>hdfs://NAMENODE_HOSTNAME:8020/ranger/audit</value>
+    <description>HDFS folder to write audit to, make sure the service user has requried permissions</description>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.hdfs.dir</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.batch.filespool.dir</name>
+    <value>/var/log/knox/audit/hdfs/spool</value>
+    <description>/var/log/knox/audit/hdfs/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr</name>
+    <value>false</value>
+    <display-name>Audit to SOLR</display-name>
+    <description>Is Solr audit enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.solr</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.urls</name>
+    <value/>
+    <description>Solr URL</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.urls</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.zookeepers</name>
+    <value>NONE</value>
+    <description>Solr Zookeeper string</description>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.zookeepers</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.batch.filespool.dir</name>
+    <value>/var/log/knox/audit/solr/spool</value>
+    <description>/var/log/knox/audit/solr/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.provider.summary.enabled</name>
+    <value>false</value>
+    <display-name>Audit provider summary enabled</display-name>
+    <description>Enable Summary audit?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.plugin.knox.ambari.cluster.name</name>
+    <value>{{cluster_name}}</value>
+    <description>Capture cluster name from where Ranger knox plugin is enabled.</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/shell-log4j2.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/shell-log4j2.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/shell-log4j2.xml
new file mode 100644
--- /dev/null	(date 1765958702711)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KNOX/configuration/shell-log4j2.xml	(date 1765958702711)
@@ -0,0 +1,71 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="true">
+    <property>
+        <name>content</name>
+        <description>gateway-log4j2.xml</description>
+        <value><![CDATA[<?xml version="1.0" encoding="utf-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<Configuration>
+    <Properties>
+        <Property name="app.log.dir">${sys:launcher.dir}/../logs</Property>
+        <Property name="app.log.file">${sys:launcher.name}.log</Property>
+    </Properties>
+    <Appenders>
+        <Console name="stdout" target="SYSTEM_OUT">
+            <PatternLayout pattern="%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n" />
+        </Console>
+        <RollingFile name="drfa" fileName="${app.log.dir}/${app.log.file}" filePattern="${app.log.dir}/${app.log.file}.%d{yyyy-MM-dd}">
+            <PatternLayout pattern="%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n" />
+            <TimeBasedTriggeringPolicy />
+        </RollingFile>
+    </Appenders>
+    <Loggers>
+        <Root level="ERROR">
+            <AppenderRef ref="drfa" />
+        </Root>
+    </Loggers>
+</Configuration>
+
+]]>
+        </value>
+        <value-attributes>
+            <type>content</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/REDIS/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/REDIS/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/REDIS/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/REDIS/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/REDIS/metainfo.xml	(date 1765958705437)
@@ -22,9 +22,7 @@
             <!-- Redis 集群服务的基本信息 -->
             <name>REDIS</name>
             <displayName>Redis</displayName>
-            <comment>
-                Component Redis Power By JaneTTR . mail: 3832514048@qq.com ,git: https://gitee.com/tt-bigdata/ambari-env
-            </comment>
+            <comment>【增强】高性能内存键值存储服务，用于缓存、会话管理及实时数据访问，使用前需安装并部署相关运行环境及持久化存储组件</comment>
             <version>7.4.0</version>
 
             <!-- Redis 集群组件定义 -->
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml	(date 1765958704572)
@@ -21,9 +21,7 @@
     <service>
       <name>HBASE</name>
       <displayName>HBase</displayName>
-      <comment>Non-relational distributed database and centralized service for configuration management &amp;
-        synchronization
-      </comment>
+      <comment>【基础】分布式列式存储数据库，用于海量数据的实时读写与随机访问，使用前需安装并部署 HDFS、ZooKeeper 等依赖组件</comment>
       <version>2.4.13</version>
       <components>
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/LIVY/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/LIVY/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/LIVY/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/LIVY/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/LIVY/metainfo.xml	(date 1765958703311)
@@ -22,8 +22,8 @@
     <service>
       <name>LIVY</name>
       <displayName>Livy</displayName>
-      <comment>Apache Livy is a service that enables easy interaction with a Spark cluster over a REST interface. </comment>
-      <version>0.7.1-1</version>
+      <comment>【增强】Spark 作业提交与管理服务，提供基于 REST 的 Spark 任务交互接口，使用前需安装并部署 Spark 及相关运行环境</comment>
+      <version>0.7.1</version>
       <components>
         <component>
           <name>LIVY_SERVER</name>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ALLUXIO/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ALLUXIO/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ALLUXIO/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ALLUXIO/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ALLUXIO/metainfo.xml	(date 1765958706766)
@@ -22,8 +22,8 @@
     <service>
       <name>ALLUXIO</name>
       <displayName>Alluxio</displayName>
-      <comment>Alluxio is world’s first open source data orchestration technology for analytics and AI for the cloud. This service is &lt;b&gt;Technical Preview&lt;/b&gt;.</comment>
-      <version>2.9.3</version>
+      <comment>【增强】用于分析与 AI 场景的数据编排组件，提供跨存储系统的数据统一访问与性能优化能力</comment>
+      <version>2.9.4</version>
       <components>        
         <component>
           <name>ALLUXIO_MASTER</name>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TEZ/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TEZ/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TEZ/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TEZ/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/TEZ/metainfo.xml	(date 1765958699632)
@@ -21,7 +21,7 @@
     <service>
       <name>TEZ</name>
       <displayName>Tez</displayName>
-      <comment>Tez is the next generation Hadoop Query Processing framework written on top of YARN.</comment>
+      <comment>【基础】面向 Hadoop 生态的高性能 DAG 计算引擎，用于加速 Hive 等批处理作业执行，使用前需安装并部署 YARN、HDFS 等依赖组件</comment>
       <version>0.10.1</version>
       <components>
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI_INFRA_SOLR/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI_INFRA_SOLR/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI_INFRA_SOLR/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI_INFRA_SOLR/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/AMBARI_INFRA_SOLR/metainfo.xml	(date 1765958708013)
@@ -21,6 +21,7 @@
     <service>
       <name>AMBARI_INFRA_SOLR</name>
       <extends>common-services/AMBARI_INFRA_SOLR/3.0.0</extends>
+      <comment>【废弃】为避免维护多套 Solr 环境，（建议统一复用现有 Solr）</comment>
     </service>
   </services>
 </metainfo>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SPARK/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SPARK/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SPARK/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SPARK/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SPARK/metainfo.xml	(date 1765958705507)
@@ -22,7 +22,7 @@
     <service>
       <name>SPARK</name>
       <displayName>Spark</displayName>
-      <comment>Apache Spark is a unified analytics engine for large-scale data processing.</comment>
+      <comment>【增强】分布式内存计算引擎，用于批处理、流处理与机器学习等计算场景，使用前需安装并部署 Java、HDFS、Hive Metastore 等依赖组件</comment>
       <version>3.5.5</version>
       <components>
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json	(date 1765958708035)
@@ -1050,7 +1050,7 @@
         }
       },
       "HUE": {
-	     "HUE": {
+        "HUE": {
           "STACK-SELECT-PACKAGE": "hue",
           "INSTALL": [
             "hue"
@@ -1062,7 +1062,7 @@
             "hue"
           ]
         },
-		"HUE_SERVER": {
+        "HUE_SERVER": {
           "STACK-SELECT-PACKAGE": "hue",
           "INSTALL": [
             "hue"
@@ -1074,7 +1074,21 @@
             "hue"
           ]
         }
-	  }
+      },
+      "KNOX": {
+        "KNOX_GATEWAY": {
+          "STACK-SELECT-PACKAGE": "knox-server",
+          "INSTALL": [
+            "knox-server"
+          ],
+          "PATCH": [
+            "knox-server"
+          ],
+          "STANDARD": [
+            "knox-server"
+          ]
+        }
+      }
     },
     "conf-select": {
       "hadoop": [
@@ -1315,12 +1329,19 @@
           "component": "superset"
         }
       ],
-       "hue": [
+      "hue": [
         {
           "conf_dir": "/etc/hue/conf",
           "current_dir": "{0}/current/hue/desktop/conf",
           "component": "hue"
         }
+      ],
+      "knox": [
+        {
+          "conf_dir": "/etc/knox/conf",
+          "current_dir": "{0}/current/knox-server/conf",
+          "component": "knox-server"
+        }
       ]
     },
     "conf-select-patching": {
@@ -1438,14 +1459,25 @@
         ]
       },
       "ATLAS": {
-        "packages": ["atlas"]
+        "packages": [
+          "atlas"
+        ]
       },
       "HUE": {
-        "packages": ["hue"]
+        "packages": [
+          "hue"
+        ]
+      },
+      "KNOX": {
+        "packages": [
+          "knox"
+        ]
       }
     },
     "upgrade-dependencies": {
-      "ATLAS": ["STORM"],
+      "ATLAS": [
+        "STORM"
+      ],
       "HIVE": [
         "TEZ",
         "MAPREDUCE2",
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/metainfo.xml	(date 1765958707504)
@@ -24,7 +24,7 @@
     <service>
       <name>RANGER_KMS</name>
       <displayName>Ranger KMS</displayName>
-      <comment>Component Ranger KMS Power By JaneTTR . mail: 3832514048@qq.com ,git: https://gitee.com/tt-bigdata/ambari-env</comment>
+      <comment>【废弃】临时废弃，待思考完毕，后续可能将以全新姿势与大家见面！</comment>
       <version>2.4.0</version>
       <components>
 
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ATLAS/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ATLAS/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ATLAS/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ATLAS/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ATLAS/metainfo.xml	(date 1765958704193)
@@ -21,9 +21,7 @@
         <service>
             <name>ATLAS</name>
             <displayName>Atlas</displayName>
-            <comment>Component ATLAS Integrated By JaneTTR . For commercial use, please contact mail:
-                3832514048@qq.com
-            </comment>
+            <comment>【增强】元数据管理与数据治理服务，用于数据目录、血缘追踪与权限策略管理，使用前需安装并部署 Kafka、ZooKeeper、HBase、Solr 等依赖组件</comment>
             <version>2.4.0</version>
 
             <components>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/IMPALA/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/IMPALA/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/IMPALA/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/IMPALA/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/IMPALA/metainfo.xml	(date 1765958706410)
@@ -6,7 +6,7 @@
         <service>
             <name>IMPALA</name>
             <displayName>Impala</displayName>
-            <comment>Component IMPALA Integrated By JaneTTR . For commercial use, please contact mail: 3832514048@qq.com</comment>
+            <comment>【增强】面向大数据场景的 MPP SQL 查询引擎，用于对 HDFS、HBase 等存储进行低延迟交互式分析，使用前需安装并部署 HDFS、Hive Metastore 等依赖组件</comment>
             <version>4.4.1</version>
             <enabled>false</enabled>
             <components>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/metainfo.xml	(date 1765958703390)
@@ -21,7 +21,7 @@
     <service>
       <name>SOLR</name>
       <displayName>Solr</displayName>
-      <comment>Solr is the popular, blazing-fast, open source enterprise search platform built on Apache Lucene.</comment>
+      <comment>【基础】分布式搜索与索引服务，用于全文检索与结构化数据查询，使用前需安装并部署 ZooKeeper 等依赖组件</comment>
       <version>8.11.2</version>
       <components>
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CELEBORN/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CELEBORN/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CELEBORN/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CELEBORN/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/CELEBORN/metainfo.xml	(date 1765958707055)
@@ -22,7 +22,7 @@
     <service>
       <name>CELEBORN</name>
       <displayName>Celeborn</displayName>
-      <comment>Component CELEBORN Integrated By JaneTTR . For commercial use, please contact mail: 3832514048@qq.com</comment>
+      <comment>【增强】面向 Spark 的分布式 Shuffle 服务，用于替代和加速 Spark Shuffle 过程，使用前需安装并部署 Spark、HDFS、ZooKeeper 等依赖组件</comment>
       <version>0.5.3</version>
       <components>
         <component>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SQOOP/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SQOOP/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SQOOP/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SQOOP/metainfo.xml	(revision 51e5cedef23ec97dc1bb2b915646242b17bce6da)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SQOOP/metainfo.xml	(date 1765958705958)
@@ -21,8 +21,7 @@
     <service>
       <name>SQOOP</name>
       <displayName>Sqoop</displayName>
-      <comment>Component Sqoop Power By JaneTTR . mail: 3832514048@qq.com ,git: https://gitee.com/tt-bigdata/ambari-env
-      </comment>
+      <comment>【基础】关系型数据库与大数据平台之间的数据同步工具，用于批量导入导出数据，使用前需安装并部署相关数据库及 Hadoop 运行环境</comment>
       <version>1.4.7</version>
 
       <components>
